{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0735fb",
   "metadata": {},
   "source": [
    "# 顯示cuda可用性以及顯示GPU型號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad3c41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA狀態：可用\n",
      "CUDA 0:NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    print(\"CUDA狀態：可用\") \n",
    "else: \n",
    "      print(\"CUDA狀態：不可用\")\n",
    "\n",
    "print(\"CUDA 0:\"+ torch.cuda.get_device_name(0))\n",
    "# print(\"CUDA 1:\"+ torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec92686c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN套件版本：8700\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDNN套件版本：\" + str(torch.backends.cudnn.version()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887d15ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH & CUDA版本：2.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"PYTORCH & CUDA版本：\" + str(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba090530",
   "metadata": {},
   "source": [
    "# 查看誰在偷吃我的顯存！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5f1e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device('cuda:0'):\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42365ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 1                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device('cuda:1'):\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e48ce",
   "metadata": {},
   "source": [
    "# 查看顯卡詳細訊息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe4daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 29 21:34:57 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.45                 Driver Version: 536.45       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   59C    P8               2W / 111W |      0MiB /  8188MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19beb6b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi(1)                       NVIDIA                       nvidia-smi(1)\n",
      "\n",
      "NAME\n",
      "       nvidia-smi - NVIDIA System Management Interface program\n",
      "\n",
      "SYNOPSIS\n",
      "       nvidia-smi [OPTION1 [ARG1]] [OPTION2 [ARG2]] ...\n",
      "\n",
      "DESCRIPTION\n",
      "       nvidia-smi (also NVSMI) provides monitoring and management capabilities\n",
      "       for each of NVIDIA's Tesla, Quadro, GRID and GeForce devices from Fermi\n",
      "       and higher architecture families. GeForce Titan series devices are sup‐\n",
      "       ported for most functions with very limited  information  provided  for\n",
      "       the  remainder  of  the  Geforce brand.  NVSMI is a cross platform tool\n",
      "       that supports all standard NVIDIA driver-supported  Linux  distros,  as\n",
      "       well as 64bit versions of Windows starting with Windows Server 2008 R2.\n",
      "       Metrics can be consumed directly by users via stdout,  or  provided  by\n",
      "       file via CSV and XML formats for scripting purposes.\n",
      "\n",
      "       Note  that much of the functionality of NVSMI is provided by the under‐\n",
      "       lying NVML C-based library.  See the NVIDIA developer website link  be‐\n",
      "       low  for  more  information about NVML.  NVML-based python bindings are\n",
      "       also available.\n",
      "\n",
      "       The output of NVSMI is not guaranteed to be backwards compatible.  How‐\n",
      "       ever,  both  NVML and the Python bindings are backwards compatible, and\n",
      "       should be the first choice when writing any tools that  must  be  main‐\n",
      "       tained across NVIDIA driver releases.\n",
      "\n",
      "       NVML SDK: http://developer.nvidia.com/nvidia-management-library-nvml/\n",
      "\n",
      "       Python bindings: http://pypi.python.org/pypi/nvidia-ml-py/\n",
      "\n",
      "OPTIONS\n",
      "   GENERAL OPTIONS\n",
      "   -h, --help\n",
      "       Print usage information and exit.\n",
      "\n",
      "   SUMMARY OPTIONS\n",
      "   -L, --list-gpus\n",
      "       List each of the NVIDIA GPUs in the system, along with their UUIDs.\n",
      "\n",
      "   -B, --list-excluded-gpus\n",
      "       List  each  of the excluded NVIDIA GPUs in the system, along with their\n",
      "       UUIDs.\n",
      "\n",
      "   QUERY OPTIONS\n",
      "   -q, --query\n",
      "       Display GPU or Unit info.  Displayed info includes all data  listed  in\n",
      "       the  (GPU  ATTRIBUTES)  or (UNIT ATTRIBUTES) sections of this document.\n",
      "       Some devices and/or environments don't support  all  possible  informa‐\n",
      "       tion.   Any unsupported data is indicated by a \"N/A\" in the output.  By\n",
      "       default information for all available GPUs or Units is displayed.   Use\n",
      "       the -i option to restrict the output to a single GPU or Unit.\n",
      "\n",
      "   [plus optional]\n",
      "   -u, --unit\n",
      "       Display Unit data instead of GPU data.  Unit data is only available for\n",
      "       NVIDIA S-class Tesla enclosures.\n",
      "\n",
      "   -i, --id=ID\n",
      "       Display data for a single specified GPU or Unit.  The specified id  may\n",
      "       be  the GPU/Unit's 0-based index in the natural enumeration returned by\n",
      "       the driver, the GPU's board serial number, the GPU's UUID, or the GPU's\n",
      "       PCI  bus  ID (as domain:bus:device.function in hex).  It is recommended\n",
      "       that users desiring consistency use either UUID or PCI  bus  ID,  since\n",
      "       device  enumeration ordering is not guaranteed to be consistent between\n",
      "       reboots and board serial number might be shared between  multiple  GPUs\n",
      "       on the same board.\n",
      "\n",
      "   -f FILE, --filename=FILE\n",
      "       Redirect  query  output  to  the specified file in place of the default\n",
      "       stdout.  The specified file will be overwritten.\n",
      "\n",
      "   -x, --xml-format\n",
      "       Produce XML output in place of the default human-readable format.  Both\n",
      "       GPU  and  Unit  query outputs conform to corresponding DTDs.  These are\n",
      "       available via the --dtd flag.\n",
      "\n",
      "   --dtd\n",
      "       Use with -x.  Embed the DTD in the XML output.\n",
      "\n",
      "   --debug=FILE\n",
      "       Produces an encrypted debug log for use in submission of bugs  back  to\n",
      "       NVIDIA.\n",
      "\n",
      "   -d TYPE, --display=TYPE\n",
      "       Display  only  selected information: MEMORY, UTILIZATION, ECC, TEMPERA‐\n",
      "       TURE,  POWER,  CLOCK,  COMPUTE,  PIDS,  PERFORMANCE,  SUPPORTED_CLOCKS,\n",
      "       PAGE_RETIREMENT,   ACCOUNTING,  ENCODER_STATS,  ROW_REMAPPER,  VOLTAGE.\n",
      "       Flags can be combined with comma  e.g.   \"MEMORY,ECC\".   Sampling  data\n",
      "       with max, min and avg is also returned for POWER, UTILIZATION and CLOCK\n",
      "       display types.  Doesn't work with -u/--unit or -x/--xml-format flags.\n",
      "\n",
      "   -l SEC, --loop=SEC\n",
      "       Continuously report query data at the specified interval,  rather  than\n",
      "       the  default  of  just  once.   The  application  will sleep in-between\n",
      "       queries.  Note that on Linux ECC error or XID error events  will  print\n",
      "       out during the sleep period if the -x flag was not specified.  Pressing\n",
      "       Ctrl+C at any time will abort the loop, which will otherwise run indef‐\n",
      "       initely.   If no argument is specified for the -l form a default inter‐\n",
      "       val of 5 seconds is used.\n",
      "\n",
      "   SELECTIVE QUERY OPTIONS\n",
      "       Allows the caller to pass an explicit list of properties to query.\n",
      "\n",
      "   [one of]\n",
      "   --query-gpu=\n",
      "       Information about GPU.  Pass comma separated  list  of  properties  you\n",
      "       want  to  query.   e.g.  --query-gpu=pci.bus_id,persistence_mode.  Call\n",
      "       --help-query-gpu for more info.\n",
      "\n",
      "   --query-supported-clocks=\n",
      "       List of supported clocks.  Call --help-query-supported-clocks for  more\n",
      "       info.\n",
      "\n",
      "   --query-compute-apps=\n",
      "       List  of  currently  active  compute processes.  Call --help-query-com‐\n",
      "       pute-apps for more info.\n",
      "\n",
      "   --query-accounted-apps=\n",
      "       List of accounted compute processes.  Call  --help-query-accounted-apps\n",
      "       for more info.  This query is not supported on vGPU host.\n",
      "\n",
      "   --query-retired-pages=\n",
      "       List  of  GPU  device  memory  pages  that  have  been  retired.   Call\n",
      "       --help-query-retired-pages for more info.\n",
      "\n",
      "   --query-remapped-rows=\n",
      "       Information about remapped rows.  Call  --help-query-remapped-rows  for\n",
      "       more info.\n",
      "\n",
      "   [mandatory]\n",
      "   --format=\n",
      "       Comma separated list of format options:\n",
      "\n",
      "       •      csv - comma separated values (MANDATORY)\n",
      "\n",
      "       •      noheader - skip first line with column headers\n",
      "\n",
      "       •      nounits - don't print units for numerical values\n",
      "\n",
      "   [plus any of]\n",
      "   -i, --id=ID\n",
      "       Display  data  for a single specified GPU.  The specified id may be the\n",
      "       GPU's 0-based index in the natural enumeration returned by the  driver,\n",
      "       the  GPU's board serial number, the GPU's UUID, or the GPU's PCI bus ID\n",
      "       (as domain:bus:device.function in hex).  It is recommended  that  users\n",
      "       desiring  consistency  use either UUID or PCI bus ID, since device enu‐\n",
      "       meration ordering is not guaranteed to be  consistent  between  reboots\n",
      "       and  board  serial  number might be shared between multiple GPUs on the\n",
      "       same board.\n",
      "\n",
      "   -f FILE, --filename=FILE\n",
      "       Redirect query output to the specified file in  place  of  the  default\n",
      "       stdout.  The specified file will be overwritten.\n",
      "\n",
      "   -l SEC, --loop=SEC\n",
      "       Continuously  report  query data at the specified interval, rather than\n",
      "       the default of  just  once.   The  application  will  sleep  in-between\n",
      "       queries.   Note  that on Linux ECC error or XID error events will print\n",
      "       out during the sleep period if the -x flag was not specified.  Pressing\n",
      "       Ctrl+C at any time will abort the loop, which will otherwise run indef‐\n",
      "       initely.  If no argument is specified for the -l form a default  inter‐\n",
      "       val of 5 seconds is used.\n",
      "\n",
      "   -lms ms, --loop-ms=ms\n",
      "       Same as -l,--loop but in milliseconds.\n",
      "\n",
      "   DEVICE MODIFICATION OPTIONS\n",
      "   [any one of]\n",
      "   -pm, --persistence-mode=MODE\n",
      "       Set the persistence mode for the target GPUs.  See the (GPU ATTRIBUTES)\n",
      "       section for a description of persistence mode.   Requires  root.   Will\n",
      "       impact all GPUs unless a single GPU is specified using the -i argument.\n",
      "       The effect of this operation is immediate.  However, it does  not  per‐\n",
      "       sist  across  reboots.  After each reboot persistence mode will default\n",
      "       to \"Disabled\".  Available on Linux only.\n",
      "\n",
      "   -e, --ecc-config=CONFIG\n",
      "       Set the ECC mode for the target GPUs.  See the (GPU ATTRIBUTES) section\n",
      "       for  a  description  of ECC mode.  Requires root.  Will impact all GPUs\n",
      "       unless a single GPU is specified using the -i argument.   This  setting\n",
      "       takes effect after the next reboot and is persistent.\n",
      "\n",
      "   -p, --reset-ecc-errors=TYPE\n",
      "       Reset  the  ECC  error  counters for the target GPUs.  See the (GPU AT‐\n",
      "       TRIBUTES) section for a description of ECC error counter types.  Avail‐\n",
      "       able arguments are 0|VOLATILE or 1|AGGREGATE.  Requires root.  Will im‐\n",
      "       pact all GPUs unless a single GPU is specified using the  -i  argument.\n",
      "       The effect of this operation is immediate.\n",
      "\n",
      "   -c, --compute-mode=MODE\n",
      "       Set  the  compute  mode  for the target GPUs.  See the (GPU ATTRIBUTES)\n",
      "       section for a description of compute mode.  Requires root.  Will impact\n",
      "       all  GPUs  unless a single GPU is specified using the -i argument.  The\n",
      "       effect of this operation is immediate.  However, it  does  not  persist\n",
      "       across  reboots.   After  each  reboot  compute mode will reset to \"DE‐\n",
      "       FAULT\".\n",
      "\n",
      "   -dm TYPE, --driver-model=TYPE\n",
      "   -fdm TYPE, --force-driver-model=TYPE\n",
      "       Enable or disable TCC driver model.  For Windows only.  Requires admin‐\n",
      "       istrator  privileges.  -dm will fail if a display is attached, but -fdm\n",
      "       will force the driver model to change.  Will impact all GPUs  unless  a\n",
      "       single  GPU  is  specified using the -i argument.  A reboot is required\n",
      "       for the change to take place.  See Driver Model for more information on\n",
      "       Windows driver models.\n",
      "\n",
      "        --gom=MODE\n",
      "       Set  GPU  Operation  Mode:  0/ALL_ON,  1/COMPUTE, 2/LOW_DP Supported on\n",
      "       GK110 M-class and X-class Tesla products from the Kepler  family.   Not\n",
      "       supported  on Quadro and Tesla C-class products.  LOW_DP and ALL_ON are\n",
      "       the only modes supported on GeForce Titan devices.   Requires  adminis‐\n",
      "       trator  privileges.   See GPU Operation Mode for more information about\n",
      "       GOM.  GOM changes take effect after  reboot.   The  reboot  requirement\n",
      "       might  be  removed in the future.  Compute only GOMs don't support WDDM\n",
      "       (Windows Display Driver Model)\n",
      "\n",
      "   -r, --gpu-reset\n",
      "       Trigger a reset of one or more GPUs.  Can be used to clear GPU  HW  and\n",
      "       SW  state  in situations that would otherwise require a machine reboot.\n",
      "       Typically useful if a double bit ECC error has occurred.   Optional  -i\n",
      "       switch  can  be  used  to target one or more specific devices.  Without\n",
      "       this option, all GPUs are reset.  Requires root.  There  can't  be  any\n",
      "       applications  using  these devices (e.g. CUDA application, graphics ap‐\n",
      "       plication like X server, monitoring application like other instance  of\n",
      "       nvidia-smi).   There  also can't be any compute applications running on\n",
      "       any other GPU in the system.\n",
      "\n",
      "       Starting with the NVIDIA Ampere architecture, GPUs with NVLink  connec‐\n",
      "       tions  can  be individually reset.  On NVSwitch systems, Fabric Manager\n",
      "       is required to facilitate reset.\n",
      "\n",
      "       If Fabric Manager is not running, or if any of the GPUs being reset are\n",
      "       based  on an architecture preceding the NVIDIA Ampere architecture, any\n",
      "       GPUs with NVLink connections to a GPU being reset must also be reset in\n",
      "       the  same  command.  This can be done either by omitting the -i switch,\n",
      "       or using the -i switch to specify the GPUs to be reset.  If the -i  op‐\n",
      "       tion does not specify a complete set of NVLink GPUs to reset, this com‐\n",
      "       mand will issue an error identifying the additional GPUs that  must  be\n",
      "       included in the reset command.\n",
      "\n",
      "       GPU reset is not guaranteed to work in all cases. It is not recommended\n",
      "       for production environments at this time.  In some situations there may\n",
      "       be  HW  components  on the board that fail to revert back to an initial\n",
      "       state following the reset request.  This is more likely to be  seen  on\n",
      "       Fermi-generation products vs. Kepler, and more likely to be seen if the\n",
      "       reset is being performed on a hung GPU.\n",
      "\n",
      "       Following a reset, it is recommended that the health of each reset  GPU\n",
      "       be  verified  before further use.  If any GPU is not healthy a complete\n",
      "       reset should be instigated by power cycling the node.\n",
      "\n",
      "       GPU reset operation will not be supported on MIG enabled vGPU guests.\n",
      "\n",
      "       Visit http://developer.nvidia.com/gpu-deployment-kit  to  download  the\n",
      "       GDK.\n",
      "\n",
      "   -lgc, --lock-gpu-clocks=MIN_GPU_CLOCK,MAX_GPU_CLOCK\n",
      "       Specifies  <minGpuClock,maxGpuClock>  clocks as a pair (e.g. 1500,1500)\n",
      "       that defines closest desired locked GPU clock speed in MHz.  Input  can\n",
      "       also  use  be  a  singular  desired clock value (e.g. <GpuClockValue>).\n",
      "       Supported on Volta+.  Requires root\n",
      "\n",
      "   -rgc, --reset-gpu-clocks\n",
      "       Resets the GPU clocks to the default value.  Supported on Volta+.   Re‐\n",
      "       quires root.\n",
      "\n",
      "   -ac, --applications-clocks=MEM_CLOCK,GRAPHICS_CLOCK\n",
      "       Specifies  maximum  <memory,graphics>  clocks as a pair (e.g. 2000,800)\n",
      "       that defines GPU's speed while running applications  on  a  GPU.   Sup‐\n",
      "       ported  on  Maxwell-based  GeForce  and  from  the  Kepler+  family  in\n",
      "       Tesla/Quadro/Titan devices.  Requires root.\n",
      "\n",
      "   -rac, --reset-applications-clocks\n",
      "       Resets the applications clocks to the default value.  Supported on Max‐\n",
      "       well-based  GeForce  and  from the Kepler+ family in Tesla/Quadro/Titan\n",
      "       devices.  Requires root.\n",
      "\n",
      "   -pl, --power-limit=POWER_LIMIT\n",
      "       Specifies maximum power limit in watts.  Accepts integer  and  floating\n",
      "       point numbers.  Only on supported devices from Kepler family.  Requires\n",
      "       administrator privileges.  Value needs to be between Min and Max  Power\n",
      "       Limit as reported by nvidia-smi.\n",
      "\n",
      "   -cc, --cuda-clocks=MODE\n",
      "       Overrides or restores default CUDA clocks Available arguments are 0|RE‐\n",
      "       STORE_DEFAULT or 1|OVERRIDE.\n",
      "\n",
      "   -am, --accounting-mode=MODE\n",
      "       Enables or disables GPU Accounting.  With GPU Accounting one  can  keep\n",
      "       track  of  usage  of resources throughout lifespan of a single process.\n",
      "       Only on supported devices from Kepler family.   Requires  administrator\n",
      "       privileges.  Available arguments are 0|DISABLED or 1|ENABLED.\n",
      "\n",
      "   -caa, --clear-accounted-apps\n",
      "       Clears  all processes accounted so far.  Only on supported devices from\n",
      "       Kepler family.  Requires administrator privileges.\n",
      "\n",
      "        --auto-boost-default=MODE\n",
      "       Set the default auto boost policy to 0/DISABLED or 1/ENABLED, enforcing\n",
      "       the  change  only after the last boost client has exited.  Only on cer‐\n",
      "       tain Tesla devices from the Kepler+ family  and  Maxwell-based  GeForce\n",
      "       devices.  Requires root.\n",
      "\n",
      "        --auto-boost-default-force=MODE\n",
      "       Set the default auto boost policy to 0/DISABLED or 1/ENABLED, enforcing\n",
      "       the change immediately.  Only on certain Tesla devices from the Kepler+\n",
      "       family and Maxwell-based GeForce devices.  Requires root.\n",
      "\n",
      "        --auto-boost-permission=MODE\n",
      "       Allow non-admin/root control over auto boost mode.  Available arguments\n",
      "       are 0|UNRESTRICTED, 1|RESTRICTED.  Only on certain Tesla  devices  from\n",
      "       the Kepler+ family and Maxwell-based GeForce devices.  Requires root.\n",
      "\n",
      "   -mig, --multi-instance-gpu=MODE\n",
      "       Enables or disables Multi Instance GPU mode.  Only supported on devices\n",
      "       based on the NVIDIA Ampere architecture.  Requires root.  Available ar‐\n",
      "       guments are 0|DISABLED or 1|ENABLED.\n",
      "\n",
      "   [plus optional]\n",
      "   -i, --id=ID\n",
      "       Modify  a single specified GPU.  The specified id may be the GPU/Unit's\n",
      "       0-based index in the natural enumeration returned by  the  driver,  the\n",
      "       GPU's  board serial number, the GPU's UUID, or the GPU's PCI bus ID (as\n",
      "       domain:bus:device.function in hex).  It is recommended that  users  de‐\n",
      "       siring consistency use either UUID or PCI bus ID, since device enumera‐\n",
      "       tion ordering is not guaranteed to be consistent  between  reboots  and\n",
      "       board  serial  number might be shared between multiple GPUs on the same\n",
      "       board.\n",
      "\n",
      "   UNIT MODIFICATION OPTIONS\n",
      "   -t, --toggle-led=STATE\n",
      "       Set the LED indicator state on the front and back of the  unit  to  the\n",
      "       specified  color.   See the (UNIT ATTRIBUTES) section for a description\n",
      "       of the LED states.  Allowed colors are 0|GREEN and  1|AMBER.   Requires\n",
      "       root.\n",
      "\n",
      "   [plus optional]\n",
      "   -i, --id=ID\n",
      "       Modify a single specified Unit.  The specified id is the Unit's 0-based\n",
      "       index in the natural enumeration returned by the driver.\n",
      "\n",
      "   SHOW DTD OPTIONS\n",
      "   --dtd\n",
      "       Display Device or Unit DTD.\n",
      "\n",
      "   [plus optional]\n",
      "   -f FILE, --filename=FILE\n",
      "       Redirect query output to the specified file in  place  of  the  default\n",
      "       stdout.  The specified file will be overwritten.\n",
      "\n",
      "   -u, --unit\n",
      "       Display Unit DTD instead of device DTD.\n",
      "\n",
      "   stats\n",
      "       Display  statistics  information  about the GPU.  Use \"nvidia-smi stats\n",
      "       -h\" for more information.  Linux only.\n",
      "\n",
      "   topo\n",
      "       Display topology information about the system.   Use  \"nvidia-smi  topo\n",
      "       -h\"  for more information.  Linux only.  Shows all GPUs NVML is able to\n",
      "       detect but CPU and NUMA node affinity information will  only  be  shown\n",
      "       for  GPUs with Kepler or newer architectures.  Note: GPU enumeration is\n",
      "       the same as NVML.\n",
      "\n",
      "   drain\n",
      "       Display and modify the GPU drain states.  Use \"nvidia-smi drain -h\" for\n",
      "       more information. Linux only.\n",
      "\n",
      "   nvlink\n",
      "       Display nvlink information.  Use \"nvidia-smi nvlink -h\" for more infor‐\n",
      "       mation.\n",
      "\n",
      "   clocks\n",
      "       Query and control clocking behavior. Currently, this only  pertains  to\n",
      "       synchronized  boost.  Use  \"nvidia-smi clocks --help\" for more informa‐\n",
      "       tion.\n",
      "\n",
      "   vgpu\n",
      "       Display information on GRID virtual GPUs. Use \"nvidia-smi vgpu -h\"  for\n",
      "       more information.\n",
      "\n",
      "   mig\n",
      "       Provides controls for MIG management.\n",
      "\n",
      "   boost-slider\n",
      "       Provides controls for boost sliders management.\n",
      "\n",
      "   power-hint\n",
      "       Provides queries for power hint.\n",
      "\n",
      "RETURN VALUE\n",
      "       Return code reflects whether the operation succeeded or failed and what\n",
      "       was the reason of failure.\n",
      "\n",
      "       •      Return code 0 - Success\n",
      "\n",
      "       •      Return code 2 - A supplied argument or flag is invalid\n",
      "\n",
      "       •      Return code 3 - The requested operation is not available on tar‐\n",
      "              get device\n",
      "\n",
      "       •      Return code 4 - The current user does not have permission to ac‐\n",
      "              cess this device or perform this operation\n",
      "\n",
      "       •      Return code 6 - A query to find an object was unsuccessful\n",
      "\n",
      "       •      Return code 8 - A device's external power cables are  not  prop‐\n",
      "              erly attached\n",
      "\n",
      "       •      Return code 9 - NVIDIA driver is not loaded\n",
      "\n",
      "       •      Return  code 10 - NVIDIA Kernel detected an interrupt issue with\n",
      "              a GPU\n",
      "\n",
      "       •      Return code 12 - NVML Shared Library couldn't be found or loaded\n",
      "\n",
      "       •      Return code 13 - Local version of NVML  doesn't  implement  this\n",
      "              function\n",
      "\n",
      "       •      Return code 14 - infoROM is corrupted\n",
      "\n",
      "       •      Return code 15 - The GPU has fallen off the bus or has otherwise\n",
      "              become inaccessible\n",
      "\n",
      "       •      Return code 255 - Other error or internal driver error occurred\n",
      "\n",
      "GPU ATTRIBUTES\n",
      "       The following list describes all possible data returned by the  -q  de‐\n",
      "       vice  query  option.   Unless otherwise noted all numerical results are\n",
      "       base 10 and unitless.\n",
      "\n",
      "   Timestamp\n",
      "       The current system timestamp at the time nvidia-smi was invoked.   For‐\n",
      "       mat is \"Day-of-week Month Day HH:MM:SS Year\".\n",
      "\n",
      "   Driver Version\n",
      "       The  version  of  the  installed NVIDIA display driver.  This is an al‐\n",
      "       phanumeric string.\n",
      "\n",
      "   Attached GPUs\n",
      "       The number of NVIDIA GPUs in the system.\n",
      "\n",
      "   Product Name\n",
      "       The official product name of the GPU.  This is an alphanumeric  string.\n",
      "       For all products.\n",
      "\n",
      "   Display Mode\n",
      "       A flag that indicates whether a physical display (e.g. monitor) is cur‐\n",
      "       rently connected to any of the GPU's connectors.   \"Enabled\"  indicates\n",
      "       an attached display.  \"Disabled\" indicates otherwise.\n",
      "\n",
      "   Display Active\n",
      "       A  flag  that  indicates  whether a display is initialized on the GPU's\n",
      "       (e.g. memory is allocated on the device for display).  Display  can  be\n",
      "       active  even  when  no monitor is physically attached.  \"Enabled\" indi‐\n",
      "       cates an active display.  \"Disabled\" indicates otherwise.\n",
      "\n",
      "   Persistence Mode\n",
      "       A flag that indicates whether persistence mode is enabled for the  GPU.\n",
      "       Value  is either \"Enabled\" or \"Disabled\".  When persistence mode is en‐\n",
      "       abled the NVIDIA driver remains loaded even  when  no  active  clients,\n",
      "       such  as  X11 or nvidia-smi, exist.  This minimizes the driver load la‐\n",
      "       tency associated with running dependent apps, such  as  CUDA  programs.\n",
      "       For all CUDA-capable products.  Linux only.\n",
      "\n",
      "   Accounting Mode\n",
      "       A  flag  that  indicates whether accounting mode is enabled for the GPU\n",
      "       Value is either When accounting is enabled  statistics  are  calculated\n",
      "       for each compute process running on the GPU.  Statistics can be queried\n",
      "       during the lifetime or after termination of the process. The  execution\n",
      "       time  of process is reported as 0 while the process is in running state\n",
      "       and updated to actual execution time after the process has  terminated.\n",
      "       See --help-query-accounted-apps for more info.\n",
      "\n",
      "   Accounting Mode Buffer Size\n",
      "       Returns  the  size  of the circular buffer that holds list of processes\n",
      "       that can be queried for accounting stats.  This is the  maximum  number\n",
      "       of  processes that accounting information will be stored for before in‐\n",
      "       formation about oldest processes will get  overwritten  by  information\n",
      "       about new processes.\n",
      "\n",
      "   Driver Model\n",
      "       On  Windows,  the TCC and WDDM driver models are supported.  The driver\n",
      "       model can be changed with the (-dm) or (-fdm) flags.   The  TCC  driver\n",
      "       model  is optimized for compute applications.  I.E. kernel launch times\n",
      "       will be quicker with TCC.  The WDDM driver model is designed for graph‐\n",
      "       ics  applications  and  is  not  recommended  for compute applications.\n",
      "       Linux does not support multiple driver models, and will always have the\n",
      "       value of \"N/A\".\n",
      "\n",
      "       Current        The  driver  model  currently  in  use.  Always \"N/A\" on\n",
      "                      Linux.\n",
      "\n",
      "       Pending        The driver model that will be used on the  next  reboot.\n",
      "                      Always \"N/A\" on Linux.\n",
      "\n",
      "   Serial Number\n",
      "       This number matches the serial number physically printed on each board.\n",
      "       It is a globally unique immutable alphanumeric value.\n",
      "\n",
      "   GPU UUID\n",
      "       This value is the globally unique immutable alphanumeric identifier  of\n",
      "       the GPU.  It does not correspond to any physical label on the board.\n",
      "\n",
      "   Minor Number\n",
      "       The  minor  number  for  the device is such that the Nvidia device node\n",
      "       file for each GPU will have the form /dev/nvidia[minor number].  Avail‐\n",
      "       able only on Linux platform.\n",
      "\n",
      "   VBIOS Version\n",
      "       The BIOS of the GPU board.\n",
      "\n",
      "   MultiGPU Board\n",
      "       Whether or not this GPU is part of a multiGPU board.\n",
      "\n",
      "   Board ID\n",
      "       The  unique  board ID assigned by the driver.  If two or more GPUs have\n",
      "       the same board ID and the above \"MultiGPU\" field is true then the  GPUs\n",
      "       are on the same board.\n",
      "\n",
      "   Inforom Version\n",
      "       Version  numbers  for  each  object in the GPU board's inforom storage.\n",
      "       The inforom is a small, persistent store  of  configuration  and  state\n",
      "       data for the GPU.  All inforom version fields are numerical.  It can be\n",
      "       useful to know these version numbers because some GPU features are only\n",
      "       available with inforoms of a certain version or higher.\n",
      "\n",
      "       If any of the fields below return Unknown Error additional Inforom ver‐\n",
      "       ification check is performed and appropriate warning  message  is  dis‐\n",
      "       played.\n",
      "\n",
      "       Image Version  Global version of the infoROM image.  Image version just\n",
      "                      like VBIOS version uniquely describes the exact  version\n",
      "                      of  the  infoROM flashed on the board in contrast to in‐\n",
      "                      foROM object version which is only an indicator of  sup‐\n",
      "                      ported features.\n",
      "\n",
      "       OEM Object     Version for the OEM configuration data.\n",
      "\n",
      "       ECC Object     Version for the ECC recording data.\n",
      "\n",
      "       Power Object   Version for the power management data.\n",
      "\n",
      "   GPU Operation Mode\n",
      "       GOM  allows  to  reduce power usage and optimize GPU throughput by dis‐\n",
      "       abling GPU features.\n",
      "\n",
      "       Each GOM is designed to meet specific user needs.\n",
      "\n",
      "       In \"All On\" mode everything is enabled and running at full speed.\n",
      "\n",
      "       The \"Compute\" mode is designed for running only compute tasks. Graphics\n",
      "       operations are not allowed.\n",
      "\n",
      "       The \"Low Double Precision\" mode is designed for running graphics appli‐\n",
      "       cations that don't require high bandwidth double precision.\n",
      "\n",
      "       GOM can be changed with the (--gom) flag.\n",
      "\n",
      "       Supported on GK110 M-class and X-class Tesla products from  the  Kepler\n",
      "       family.   Not supported on Quadro and Tesla C-class products.  Low Dou‐\n",
      "       ble Precision and All On modes are the only modes  available  for  sup‐\n",
      "       ported GeForce Titan products.\n",
      "\n",
      "       Current        The GOM currently in use.\n",
      "\n",
      "       Pending        The GOM that will be used on the next reboot.\n",
      "\n",
      "   PCI\n",
      "       Basic  PCI  info  for  the device.  Some of this information may change\n",
      "       whenever cards are added/removed/moved in a system.  For all products.\n",
      "\n",
      "       Bus            PCI bus number, in hex\n",
      "\n",
      "       Device         PCI device number, in hex\n",
      "\n",
      "       Domain         PCI domain number, in hex\n",
      "\n",
      "       Device Id      PCI vendor device id, in hex\n",
      "\n",
      "       Sub System Id  PCI Sub System id, in hex\n",
      "\n",
      "       Bus Id         PCI bus id as \"domain:bus:device.function\", in hex\n",
      "\n",
      "   GPU Link information\n",
      "       The PCIe link generation and bus width\n",
      "\n",
      "       Current        The current link generation and width.  These may be re‐\n",
      "                      duced when the GPU is not in use.\n",
      "\n",
      "       Maximum        The maximum link generation and width possible with this\n",
      "                      GPU and system configuration.  For example, if  the  GPU\n",
      "                      supports  a  higher PCIe generation than the system sup‐\n",
      "                      ports then this reports the system PCIe generation.\n",
      "\n",
      "   Bridge Chip\n",
      "       Information related to Bridge Chip  on  the  device.  The  bridge  chip\n",
      "       firmware  is  only  present on certain boards and may display \"N/A\" for\n",
      "       some newer multiGPUs boards.\n",
      "\n",
      "       Type           The type of bridge chip. Reported as N/A if doesn't  ex‐\n",
      "                      ist.\n",
      "\n",
      "       Firmware Version\n",
      "                      The firmware version of the bridge chip. Reported as N/A\n",
      "                      if doesn't exist.\n",
      "\n",
      "   Replays Since Reset\n",
      "       The number of PCIe replays since reset.\n",
      "\n",
      "   Replay Number Rollovers\n",
      "       The number of PCIe replay number rollovers since reset. A replay number\n",
      "       rollover  occurs  after 4 consecutive replays and results in retraining\n",
      "       the link.\n",
      "\n",
      "   Tx Throughput\n",
      "       The GPU-centric transmission throughput across the  PCIe  bus  in  MB/s\n",
      "       over the past 20ms.  Only supported on Maxwell architectures and newer.\n",
      "\n",
      "   Rx Throughput\n",
      "       The GPU-centric receive throughput across the PCIe bus in MB/s over the\n",
      "       past 20ms.  Only supported on Maxwell architectures and newer.\n",
      "\n",
      "   Fan Speed\n",
      "       The fan speed value is the percent of the product's maximum noise  tol‐\n",
      "       erance fan speed that the device's fan is currently intended to run at.\n",
      "       This value may exceed 100% in certain cases.  Note: The reported  speed\n",
      "       is the intended fan speed.  If the fan is physically blocked and unable\n",
      "       to spin, this output will not match the actual fan speed.   Many  parts\n",
      "       do  not  report fan speeds because they rely on cooling via fans in the\n",
      "       surrounding enclosure.  For all discrete products with dedicated fans.\n",
      "\n",
      "   Performance State\n",
      "       The current performance state for the GPU.  States range from P0 (maxi‐\n",
      "       mum performance) to P12 (minimum performance).\n",
      "\n",
      "   Clocks Throttle Reasons\n",
      "       Retrieves  information about factors that are reducing the frequency of\n",
      "       clocks.\n",
      "\n",
      "       If all throttle reasons are returned as  \"Not  Active\"  it  means  that\n",
      "       clocks are running as high as possible.\n",
      "\n",
      "       Idle           Nothing  is  running on the GPU and the clocks are drop‐\n",
      "                      ping to Idle state.  This limiter may be  removed  in  a\n",
      "                      later release.\n",
      "\n",
      "       Application Clocks Setting\n",
      "                      GPU  clocks  are limited by applications clocks setting.\n",
      "                      E.g.  can  be  changed   using   nvidia-smi   --applica‐\n",
      "                      tions-clocks=\n",
      "\n",
      "       SW Power Cap   SW  Power Scaling algorithm is reducing the clocks below\n",
      "                      requested clocks because the GPU is consuming  too  much\n",
      "                      power.   E.g.  SW  power  cap  limit can be changed with\n",
      "                      nvidia-smi --power-limit=\n",
      "\n",
      "       HW Slowdown    HW Slowdown (reducing the core clocks by a factor  of  2\n",
      "                      or  more)  is engaged.  HW Thermal Slowdown and HW Power\n",
      "                      Brake will be displayed on Pascal+.\n",
      "\n",
      "                      This is an indicator of:\n",
      "                      * Temperature being too high (HW Thermal Slowdown)\n",
      "                      * External Power Brake Assertion is triggered  (e.g.  by\n",
      "                      the system power supply) (HW Power Brake Slowdown)\n",
      "                      *  Power draw is too high and Fast Trigger protection is\n",
      "                      reducing the clocks\n",
      "\n",
      "       SW Thermal Slowdown\n",
      "                      SW Thermal capping algorithm is  reducing  clocks  below\n",
      "                      requested  clocks because GPU temperature is higher than\n",
      "                      Max Operating Temp\n",
      "\n",
      "   FB Memory Usage\n",
      "       On-board frame buffer memory information.  Reported total memory is af‐\n",
      "       fected  by  ECC state.  If ECC is enabled the total available memory is\n",
      "       decreased by several percent, due to the requisite  parity  bits.   The\n",
      "       driver may also reserve a small amount of memory for internal use, even\n",
      "       without active work on the GPU.  For all products.\n",
      "\n",
      "       Total          Total size of FB memory.\n",
      "\n",
      "       Used           Used size of FB memory.\n",
      "\n",
      "       Free           Available size of FB memory.\n",
      "\n",
      "   BAR1 Memory Usage\n",
      "       BAR1 is used to map the FB (device memory) so that it can  be  directly\n",
      "       accessed  by  the CPU or by 3rd party devices (peer-to-peer on the PCIe\n",
      "       bus).\n",
      "\n",
      "       Total          Total size of BAR1 memory.\n",
      "\n",
      "       Used           Used size of BAR1 memory.\n",
      "\n",
      "       Free           Available size of BAR1 memory.\n",
      "\n",
      "   Compute Mode\n",
      "       The compute mode flag indicates whether individual or multiple  compute\n",
      "       applications may run on the GPU.\n",
      "\n",
      "       \"Default\" means multiple contexts are allowed per device.\n",
      "\n",
      "       \"Exclusive  Process\"  means only one context is allowed per device, us‐\n",
      "       able from multiple threads at a time.\n",
      "\n",
      "       \"Prohibited\" means no contexts  are  allowed  per  device  (no  compute\n",
      "       apps).\n",
      "\n",
      "       \"EXCLUSIVE_PROCESS\"  was  added  in CUDA 4.0.  Prior CUDA releases sup‐\n",
      "       ported  only  one  exclusive  mode,  which  is  equivalent  to  \"EXCLU‐\n",
      "       SIVE_THREAD\" in CUDA 4.0 and beyond.\n",
      "\n",
      "       For all CUDA-capable products.\n",
      "\n",
      "   Utilization\n",
      "       Utilization  rates  report  how  busy each GPU is over time, and can be\n",
      "       used to determine how much an application is using the GPUs in the sys‐\n",
      "       tem.\n",
      "\n",
      "       Note: During driver initialization when ECC is enabled one can see high\n",
      "       GPU and Memory Utilization readings.  This  is  caused  by  ECC  Memory\n",
      "       Scrubbing mechanism that is performed during driver initialization.\n",
      "\n",
      "       GPU            Percent of time over the past sample period during which\n",
      "                      one or more kernels was executing on the GPU.  The  sam‐\n",
      "                      ple  period  may  be between 1 second and 1/6 second de‐\n",
      "                      pending on the product.\n",
      "\n",
      "       Memory         Percent of time over the past sample period during which\n",
      "                      global  (device)  memory was being read or written.  The\n",
      "                      sample period may be between 1 second and 1/6 second de‐\n",
      "                      pending on the product.\n",
      "\n",
      "       Encoder        Percent of time over the past sample period during which\n",
      "                      the GPU's video encoder was being  used.   The  sampling\n",
      "                      rate  is  variable  and can be obtained directly via the\n",
      "                      nvmlDeviceGetEncoderUtilization() API\n",
      "\n",
      "       Decoder        Percent of time over the past sample period during which\n",
      "                      the  GPU's  video  decoder was being used.  The sampling\n",
      "                      rate is variable and can be obtained  directly  via  the\n",
      "                      nvmlDeviceGetDecoderUtilization() API\n",
      "\n",
      "   Ecc Mode\n",
      "       A  flag  that  indicates whether ECC support is enabled.  May be either\n",
      "       \"Enabled\" or \"Disabled\".  Changes to ECC mode require  a  reboot.   Re‐\n",
      "       quires Inforom ECC object version 1.0 or higher.\n",
      "\n",
      "       Current        The ECC mode that the GPU is currently operating under.\n",
      "\n",
      "       Pending        The  ECC  mode that the GPU will operate under after the\n",
      "                      next reboot.\n",
      "\n",
      "   ECC Errors\n",
      "       NVIDIA GPUs can provide error counts for various types of  ECC  errors.\n",
      "       Some  ECC  errors are either single or double bit, where single bit er‐\n",
      "       rors are corrected and double bit errors  are  uncorrectable.   Texture\n",
      "       memory errors may be correctable via resend or uncorrectable if the re‐\n",
      "       send fails.  These errors are available across two timescales (volatile\n",
      "       and  aggregate).   Single bit ECC errors are automatically corrected by\n",
      "       the HW and do not result in data corruption.  Double bit errors are de‐\n",
      "       tected  but not corrected.  Please see the ECC documents on the web for\n",
      "       information on compute application behavior when double bit errors  oc‐\n",
      "       cur.  Volatile error counters track the number of errors detected since\n",
      "       the last driver load.  Aggregate error counts persist indefinitely  and\n",
      "       thus act as a lifetime counter.\n",
      "\n",
      "       A  note  about  volatile  counts: On Windows this is once per boot.  On\n",
      "       Linux this can be more frequent.  On Linux the driver unloads  when  no\n",
      "       active  clients  exist.  Hence, if persistence mode is enabled or there\n",
      "       is always a driver client active (e.g. X11), then Linux also sees  per-\n",
      "       boot  behavior.   If not, volatile counts are reset each time a compute\n",
      "       app is run.\n",
      "\n",
      "       Tesla and Quadro products from the Fermi and Kepler family can  display\n",
      "       total ECC error counts, as well as a breakdown of errors based on loca‐\n",
      "       tion on the chip.  The locations are described  below.   Location-based\n",
      "       data  for  aggregate  error  counts requires Inforom ECC object version\n",
      "       2.0.  All other ECC counts require ECC object version 1.0.\n",
      "\n",
      "       Device Memory  Errors detected in global device memory.\n",
      "\n",
      "       Register File  Errors detected in register file memory.\n",
      "\n",
      "       L1 Cache       Errors detected in the L1 cache.\n",
      "\n",
      "       L2 Cache       Errors detected in the L2 cache.\n",
      "\n",
      "       Texture Memory Parity errors detected in texture memory.\n",
      "\n",
      "       Total          Total errors detected across entire chip. Sum of  Device\n",
      "                      Memory,  Register  File,  L1 Cache, L2 Cache and Texture\n",
      "                      Memory.\n",
      "\n",
      "   Page Retirement\n",
      "       NVIDIA GPUs can retire pages of GPU device memory when they become  un‐\n",
      "       reliable.   This  can  happen when multiple single bit ECC errors occur\n",
      "       for the same page, or on a double bit ECC error.  When a  page  is  re‐\n",
      "       tired,  the NVIDIA driver will hide it such that no driver, or applica‐\n",
      "       tion memory allocations can access it.\n",
      "\n",
      "       Double Bit ECC The number of GPU device memory pages that have been re‐\n",
      "       tired due to a double bit ECC error.\n",
      "\n",
      "       Single Bit ECC The number of GPU device memory pages that have been re‐\n",
      "       tired due to multiple single bit ECC errors.\n",
      "\n",
      "       Pending Checks if any GPU device memory pages are pending blacklist  on\n",
      "       the  next  reboot.   Pages that are retired but not yet blacklisted can\n",
      "       still be allocated, and may cause further reliability issues.\n",
      "\n",
      "   Row Remapper\n",
      "       NVIDIA GPUs can remap rows of GPU device memory when they become  unre‐\n",
      "       liable.   This can happen when a single uncorrectable ECC error or mul‐\n",
      "       tiple correctable ECC errors occur on the same  row.   When  a  row  is\n",
      "       remapped,  the  NVIDIA  driver  will remap the faulty row to a reserved\n",
      "       row.  All future accesses to the row will access the reserved  row  in‐\n",
      "       stead of the faulty row.\n",
      "\n",
      "       Correctable  Error  The  number  of rows that have been remapped due to\n",
      "       correctable ECC errors.\n",
      "\n",
      "       Uncorrectable Error The number of rows that have been remapped  due  to\n",
      "       uncorrectable ECC errors.\n",
      "\n",
      "       Pending  Indicates  whether  or not a row is pending remapping. The GPU\n",
      "       must be reset for the remapping to go into effect.\n",
      "\n",
      "       Remapping Failure Occurred Indicates whether or not a row remapping has\n",
      "       failed in the past.\n",
      "\n",
      "       Bank  Remap  Availability Histogram Each memory bank has a fixed number\n",
      "       of reserved rows that can be used for  row  remapping.   The  histogram\n",
      "       will  classify  the remap availability of each bank into Maximum, High,\n",
      "       Partial, Low and None.  Maximum availability means  that  all  reserved\n",
      "       rows are available for remapping while None means that no reserved rows\n",
      "       are available.\n",
      "\n",
      "   Temperature\n",
      "       Readings from temperature sensors on the board.  All  readings  are  in\n",
      "       degrees C.  Not all products support all reading types.  In particular,\n",
      "       products in module form factors that rely on case fans or passive cool‐\n",
      "       ing  do  not  usually  provide temperature readings.  See below for re‐\n",
      "       strictions.\n",
      "\n",
      "       GPU            Core GPU temperature.   For  all  discrete  and  S-class\n",
      "                      products.\n",
      "\n",
      "       Shutdown Temp  The temperature at which a GPU will shutdown.\n",
      "\n",
      "       Slowdown Temp  The temperature at which a GPU will begin slowing itself\n",
      "                      down through HW, in order to cool.\n",
      "\n",
      "       Max Operating Temp\n",
      "                      The temperature at which a GPU will begin slowing itself\n",
      "                      down through SW, in order to cool.\n",
      "\n",
      "   Power Readings\n",
      "       Power  readings  help  to  shed light on the current power usage of the\n",
      "       GPU, and the factors that affect that usage.  When power management  is\n",
      "       enabled the GPU limits power draw under load to fit within a predefined\n",
      "       power envelope by manipulating the current performance state.  See  be‐\n",
      "       low  for  limits  of availability.  Please note that power readings are\n",
      "       not applicable for Pascal and higher GPUs with BA sensor boards.\n",
      "\n",
      "       Power State    Power State is deprecated and has been renamed  to  Per‐\n",
      "                      formance State in 2.285.  To maintain XML compatibility,\n",
      "                      in XML  format  Performance  State  is  listed  in  both\n",
      "                      places.\n",
      "\n",
      "       Power Management\n",
      "                      A  flag  that  indicates whether power management is en‐\n",
      "                      abled.  Either \"Supported\" or \"N/A\".   Requires  Inforom\n",
      "                      PWR object version 3.0 or higher or Kepler device.\n",
      "\n",
      "       Power Draw     The  last  measured  power draw for the entire board, in\n",
      "                      watts.  Only available if power management is supported.\n",
      "                      Please  note  that  for boards without INA sensors, this\n",
      "                      refers to the power draw for the GPU and not for the en‐\n",
      "                      tire board.\n",
      "\n",
      "       Power Limit    The  software  power  limit,  in watts.  Set by software\n",
      "                      such as nvidia-smi.  Only available if power  management\n",
      "                      is  supported.   Requires Inforom PWR object version 3.0\n",
      "                      or higher or Kepler device.   On  Kepler  devices  Power\n",
      "                      Limit can be adjusted using -pl,--power-limit= switches.\n",
      "\n",
      "       Enforced Power Limit\n",
      "                      The  power  management  algorithm's  power  ceiling,  in\n",
      "                      watts.  Total board power draw  is  manipulated  by  the\n",
      "                      power management algorithm such that it stays under this\n",
      "                      value.  This limit is the minimum of various limits such\n",
      "                      as  the  software limit listed above.  Only available if\n",
      "                      power management is supported.  Requires  a  Kepler  de‐\n",
      "                      vice.   Please note that for boards without INA sensors,\n",
      "                      it is the GPU power draw that is being manipulated.\n",
      "\n",
      "       Default Power Limit\n",
      "                      The default power management algorithm's power  ceiling,\n",
      "                      in watts.  Power Limit will be set back to Default Power\n",
      "                      Limit after driver unload.  Only  on  supported  devices\n",
      "                      from Kepler family.\n",
      "\n",
      "       Min Power Limit\n",
      "                      The  minimum  value in watts that power limit can be set\n",
      "                      to.  Only on supported devices from Kepler family.\n",
      "\n",
      "       Max Power Limit\n",
      "                      The maximum value in watts that power limit can  be  set\n",
      "                      to.  Only on supported devices from Kepler family.\n",
      "\n",
      "   Clocks\n",
      "       Current  frequency at which parts of the GPU are running.  All readings\n",
      "       are in MHz.\n",
      "\n",
      "       Graphics       Current frequency of graphics (shader) clock.\n",
      "\n",
      "       SM             Current  frequency  of  SM  (Streaming   Multiprocessor)\n",
      "                      clock.\n",
      "\n",
      "       Memory         Current frequency of memory clock.\n",
      "\n",
      "       Video          Current frequency of video (encoder + decoder) clocks.\n",
      "\n",
      "   Applications Clocks\n",
      "       User specified frequency at which applications will be running at.  Can\n",
      "       be changed with [-ac | --applications-clocks] switches.\n",
      "\n",
      "       Graphics       User specified frequency of graphics (shader) clock.\n",
      "\n",
      "       Memory         User specified frequency of memory clock.\n",
      "\n",
      "   Default Applications Clocks\n",
      "       Default frequency at which applications will be running  at.   Applica‐\n",
      "       tion clocks can be changed with [-ac | --applications-clocks] switches.\n",
      "       Application clocks can be set to default using [-rac | --reset-applica‐\n",
      "       tions-clocks] switches.\n",
      "\n",
      "       Graphics       Default  frequency  of  applications  graphics  (shader)\n",
      "                      clock.\n",
      "\n",
      "       Memory         Default frequency of applications memory clock.\n",
      "\n",
      "   Max Clocks\n",
      "       Maximum frequency at which parts of the GPU are  design  to  run.   All\n",
      "       readings are in MHz.\n",
      "\n",
      "       On  GPUs  from  Fermi family current P0 clocks (reported in Clocks sec‐\n",
      "       tion) can differ from max clocks by few MHz.\n",
      "\n",
      "       Graphics       Maximum frequency of graphics (shader) clock.\n",
      "\n",
      "       SM             Maximum  frequency  of  SM  (Streaming   Multiprocessor)\n",
      "                      clock.\n",
      "\n",
      "       Memory         Maximum frequency of memory clock.\n",
      "\n",
      "       Video          Maximum frequency of video (encoder + decoder) clock.\n",
      "\n",
      "   Clock Policy\n",
      "       User-specified  settings  for  automated  clocking changes such as auto\n",
      "       boost.\n",
      "\n",
      "       Auto Boost     Indicates whether auto boost mode is  currently  enabled\n",
      "                      for  this GPU (On) or disabled for this GPU (Off). Shows\n",
      "                      (N/A) if boost is not supported. Auto boost  allows  dy‐\n",
      "                      namic  GPU clocking based on power, thermal and utiliza‐\n",
      "                      tion. When auto boost is disabled the GPU  will  attempt\n",
      "                      to  maintain clocks at precisely the Current Application\n",
      "                      Clocks settings (whenever a  CUDA  context  is  active).\n",
      "                      With  auto  boost  enabled the GPU will still attempt to\n",
      "                      maintain this floor, but will opportunistically boost to\n",
      "                      higher  clocks when power, thermal and utilization head‐\n",
      "                      room allow. This setting persists for the  life  of  the\n",
      "                      CUDA  context  for  which it was requested. Apps can re‐\n",
      "                      quest a particular mode either via  an  NVML  call  (see\n",
      "                      NVML  SDK)  or  by setting the CUDA environment variable\n",
      "                      CUDA_AUTO_BOOST.\n",
      "\n",
      "       Auto Boost Default\n",
      "                      Indicates the default setting for auto boost  mode,  ei‐\n",
      "                      ther  enabled  (On)  or  disabled  (Off). Shows (N/A) if\n",
      "                      boost is not supported. Apps will  run  in  the  default\n",
      "                      mode  if they have not explicitly requested a particular\n",
      "                      mode. Note: Auto Boost settings can only be modified  if\n",
      "                      \"Persistence Mode\" is enabled, which is NOT by default.\n",
      "\n",
      "   Supported clocks\n",
      "       List  of  possible memory and graphics clocks combinations that the GPU\n",
      "       can operate on (not taking  into  account  HW  brake  reduced  clocks).\n",
      "       These  are the only clock combinations that can be passed to --applica‐\n",
      "       tions-clocks flag.  Supported Clocks are listed only when  -q  -d  SUP‐\n",
      "       PORTED_CLOCKS switches are provided or in XML format.\n",
      "\n",
      "   Voltage\n",
      "       Current voltage reported by the GPU. All units are in mV.\n",
      "\n",
      "       Graphics       Current voltage of the graphics unit.\n",
      "\n",
      "   Processes\n",
      "       List  of  processes  having  Compute or Graphics Context on the device.\n",
      "       Compute processes are reported on all the fully supported products. Re‐\n",
      "       porting  for  Graphics  processes  is limited to the supported products\n",
      "       starting with Kepler architecture.\n",
      "\n",
      "       Each Entry is of format \"<GPU Index> <PID> <Type> <Process  Name>  <GPU\n",
      "       Memory Usage>\"\n",
      "\n",
      "       GPU Index      Represents NVML Index of the device.\n",
      "\n",
      "       PID            Represents  Process  ID corresponding to the active Com‐\n",
      "                      pute or Graphics context.\n",
      "\n",
      "       Type           Displayed as \"C\" for Compute Process, \"G\"  for  Graphics\n",
      "                      Process,  and  \"C+G\" for the process having both Compute\n",
      "                      and Graphics contexts.\n",
      "\n",
      "       Process Name   Represents process name  for  the  Compute  or  Graphics\n",
      "                      process.\n",
      "\n",
      "       GPU Memory Usage\n",
      "                      Amount of memory used on the device by the context.  Not\n",
      "                      available on Windows when running in WDDM  mode  because\n",
      "                      Windows KMD manages all the memory not NVIDIA driver.\n",
      "\n",
      "   Stats (EXPERIMENTAL)\n",
      "       List  GPU  statistics  such  as power samples, utilization samples, xid\n",
      "       events, clock change events and violation counters.\n",
      "\n",
      "       Supported on Tesla, GRID and Quadro based products under Linux.\n",
      "\n",
      "       Limited to Kepler or newer GPUs.\n",
      "\n",
      "       Displays statistics in CSV format as follows:\n",
      "\n",
      "       <GPU device index>, <metric name>, <CPU Timestamp in  us>,  <value  for\n",
      "       metric>\n",
      "\n",
      "       The metrics to display with their units are as follows:\n",
      "\n",
      "       Power samples in Watts.\n",
      "\n",
      "       GPU Temperature samples in degrees Celsius.\n",
      "\n",
      "       GPU, Memory, Encoder and Decoder utilization samples in Percentage.\n",
      "\n",
      "       Xid  error  events  reported with Xid error code. The error code is 999\n",
      "       for unknown xid error.\n",
      "\n",
      "       Processor and Memory clock changes in MHz.\n",
      "\n",
      "       Violation due to Power capping with violation time in ns. (Tesla Only)\n",
      "\n",
      "       Violation due to Thermal capping with  violation  boolean  flag  (1/0).\n",
      "       (Tesla Only)\n",
      "\n",
      "       Notes:\n",
      "\n",
      "       Any statistic preceded by \"#\" is a comment.\n",
      "\n",
      "       Non  supported device is displayed as \"#<device Index>, Device not sup‐\n",
      "       ported\".\n",
      "\n",
      "       Non supported metric is displayed as \"<device  index>,  <metric  name>,\n",
      "       N/A, N/A\".\n",
      "\n",
      "       Violation due to Thermal/Power supported only for Tesla based products.\n",
      "       Thermal Violations are limited to Tesla K20 and higher.\n",
      "\n",
      "   Device Monitoring\n",
      "       The \"nvidia-smi dmon\" command-line is used to monitor one or more  GPUs\n",
      "       (up to 4 devices) plugged into the system. This tool allows the user to\n",
      "       see one line of monitoring data per monitoring cycle. The output is  in\n",
      "       concise  format  and  easy to interpret in interactive mode. The output\n",
      "       data per line is limited by the  terminal  size.  It  is  supported  on\n",
      "       Tesla,  GRID,  Quadro  and limited GeForce products for Kepler or newer\n",
      "       GPUs under bare metal 64 bits Linux. By default,  the  monitoring  data\n",
      "       includes  Power  Usage,  Temperature, SM clocks, Memory clocks and Uti‐\n",
      "       lization values for SM, Memory, Encoder and Decoder.  It  can  also  be\n",
      "       configured  to  report other metrics such as frame buffer memory usage,\n",
      "       bar1 memory usage, power/thermal violations and aggregate single/double\n",
      "       bit  ecc errors. If any of the metric is not supported on the device or\n",
      "       any other error in fetching the metric is reported as \"-\" in the output\n",
      "       data.  The  user can also configure monitoring frequency and the number\n",
      "       of monitoring iterations for each run. There is also an option  to  in‐\n",
      "       clude  date and time at each line. All the supported options are exclu‐\n",
      "       sive and can be used together in any order.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Default with no arguments\n",
      "\n",
      "       nvidia-smi dmon\n",
      "\n",
      "       Monitors default metrics for up to 4 supported  devices  under  natural\n",
      "       enumeration  (starting  with GPU index 0) at a frequency of 1 sec. Runs\n",
      "       until terminated with ^C.\n",
      "\n",
      "       2) Select one or more devices\n",
      "\n",
      "       nvidia-smi dmon -i <device1,device2, .. , deviceN>\n",
      "\n",
      "       Reports default metrics for the devices selected by comma separated de‐\n",
      "       vice list. The tool picks up to 4 supported devices from the list under\n",
      "       natural enumeration (starting with GPU index 0).\n",
      "\n",
      "       3) Select metrics to be displayed\n",
      "\n",
      "       nvidia-smi dmon -s <metric_group>\n",
      "\n",
      "       <metric_group> can be one or more from the following:\n",
      "\n",
      "           p - Power Usage (in Watts) and Gpu/Memory  Temperature  (in  C)  if\n",
      "       supported\n",
      "\n",
      "           u - Utilization (SM, Memory, Encoder and Decoder Utilization in %)\n",
      "\n",
      "           c - Proc and Mem Clocks (in MHz)\n",
      "\n",
      "           v  -  Power  Violations (in %) and Thermal Violations (as a boolean\n",
      "       flag)\n",
      "\n",
      "           m - Frame Buffer and Bar1 memory usage (in MB)\n",
      "\n",
      "           e - ECC (Number of aggregated single bit, double  bit  ecc  errors)\n",
      "       and PCIe Replay errors\n",
      "\n",
      "           t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)\n",
      "\n",
      "       4) Configure monitoring iterations\n",
      "\n",
      "       nvidia-smi dmon -c <number of samples>\n",
      "\n",
      "       Displays data for specified number of samples and exit.\n",
      "\n",
      "       5) Configure monitoring frequency\n",
      "\n",
      "       nvidia-smi dmon -d <time in secs>\n",
      "\n",
      "       Collects and displays data at every specified monitoring interval until\n",
      "       terminated with ^C.\n",
      "\n",
      "       6) Display date\n",
      "\n",
      "       nvidia-smi dmon -o D\n",
      "\n",
      "       Prepends monitoring data with date in YYYYMMDD format.\n",
      "\n",
      "       7) Display time\n",
      "\n",
      "       nvidia-smi dmon -o T\n",
      "\n",
      "       Prepends monitoring data with time in HH:MM:SS format.\n",
      "\n",
      "       8) Help Information\n",
      "\n",
      "       nvidia-smi dmon -h\n",
      "\n",
      "       Displays help information for using the command line.\n",
      "\n",
      "   Daemon (EXPERIMENTAL)\n",
      "       The \"nvidia-smi daemon\" starts a background process to monitor  one  or\n",
      "       more  GPUs plugged in to the system. It monitors the requested GPUs ev‐\n",
      "       ery monitoring cycle and logs the file in compressed format at the user\n",
      "       provided  path  or  the  default location at /var/log/nvstats/. The log\n",
      "       file is created with system's date appended to it  and  of  the  format\n",
      "       nvstats-YYYYMMDD. The flush operation to the log file is done every al‐\n",
      "       ternate  monitoring  cycle.  Daemon  also  logs   it's   own   PID   at\n",
      "       /var/run/nvsmi.pid. By default, the monitoring data to persist includes\n",
      "       Power Usage, Temperature, SM clocks, Memory clocks and Utilization val‐\n",
      "       ues  for  SM, Memory, Encoder and Decoder. The daemon tools can also be\n",
      "       configured to record other metrics such as frame buffer  memory  usage,\n",
      "       bar1 memory usage, power/thermal violations and aggregate single/double\n",
      "       bit ecc errors.The default monitoring cycle is set to 10 secs  and  can\n",
      "       be  configured via command-line. It is supported on Tesla, GRID, Quadro\n",
      "       and GeForce products for Kepler or newer GPUs under bare metal 64  bits\n",
      "       Linux.  The  daemon requires root privileges to run, and  only supports\n",
      "       running a single instance on the system. All of the  supported  options\n",
      "       are exclusive and can be used together in any order.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Default with no arguments\n",
      "\n",
      "       nvidia-smi daemon\n",
      "\n",
      "       Runs in the background to monitor default metrics for up to 4 supported\n",
      "       devices under natural enumeration (starting with GPU index 0) at a fre‐\n",
      "       quency of 10 sec. The date stamped log file is created at /var/log/nvs‐\n",
      "       tats/.\n",
      "\n",
      "       2) Select one or more devices\n",
      "\n",
      "       nvidia-smi daemon -i <device1,device2, .. , deviceN>\n",
      "\n",
      "       Runs in the background to monitor default metrics for the  devices  se‐\n",
      "       lected by comma separated device list. The tool picks up to 4 supported\n",
      "       devices from the list under natural enumeration (starting with GPU  in‐\n",
      "       dex 0).\n",
      "\n",
      "       3) Select metrics to be monitored\n",
      "\n",
      "       nvidia-smi daemon -s <metric_group>\n",
      "\n",
      "       <metric_group> can be one or more from the following:\n",
      "\n",
      "           p  -  Power  Usage  (in Watts) and Gpu/Memory Temperature (in C) if\n",
      "       supported\n",
      "\n",
      "           u - Utilization (SM, Memory, Encoder and Decoder Utilization in %)\n",
      "\n",
      "           c - Proc and Mem Clocks (in MHz)\n",
      "\n",
      "           v - Power Violations (in %) and Thermal Violations  (as  a  boolean\n",
      "       flag)\n",
      "\n",
      "           m - Frame Buffer and Bar1 memory usage (in MB)\n",
      "\n",
      "            e  -  ECC (Number of aggregated single bit, double bit ecc errors)\n",
      "       and PCIe Replay errors\n",
      "\n",
      "           t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)\n",
      "\n",
      "       4) Configure monitoring frequency\n",
      "\n",
      "       nvidia-smi daemon -d <time in secs>\n",
      "\n",
      "       Collects data at every specified monitoring interval until terminated.\n",
      "\n",
      "       5) Configure log directory\n",
      "\n",
      "       nvidia-smi daemon -p <path of directory>\n",
      "\n",
      "       The log files are created at the specified directory.\n",
      "\n",
      "       6) Configure log file name\n",
      "\n",
      "       nvidia-smi daemon -j <string to append log file name>\n",
      "\n",
      "       The command-line is used to append the log file name with the user pro‐\n",
      "       vided string.\n",
      "\n",
      "       7) Terminate the daemon\n",
      "\n",
      "       nvidia-smi daemon -t\n",
      "\n",
      "       This command-line uses the stored PID (at /var/run/nvsmi.pid) to termi‐\n",
      "       nate the daemon. It makes the best effort to stop the daemon and offers\n",
      "       no  guarantees  for  it's termination. In case the daemon is not termi‐\n",
      "       nated, then the user can manually terminate by sending kill  signal  to\n",
      "       the  daemon. Performing a GPU reset operation (via nvidia-smi) requires\n",
      "       all GPU processes to be exited, including the daemon.  Users  who  have\n",
      "       the daemon open will see an error to the effect that the GPU is busy.\n",
      "\n",
      "       8) Help Information\n",
      "\n",
      "       nvidia-smi daemon -h\n",
      "\n",
      "       Displays help information for using the command line.\n",
      "\n",
      "   Replay Mode (EXPERIMENTAL)\n",
      "       The  \"nvidia-smi  replay\" command-line is used to extract/replay all or\n",
      "       parts of log file generated by the daemon. By default, the  tool  tries\n",
      "       to pull the metrics such as Power Usage, Temperature, SM clocks, Memory\n",
      "       clocks and Utilization values for SM, Memory, Encoder and Decoder.  The\n",
      "       replay  tool  can  also fetch other metrics such as frame buffer memory\n",
      "       usage, bar1 memory usage, power/thermal violations and  aggregate  sin‐\n",
      "       gle/double  bit  ecc errors. There is an option to select a set of met‐\n",
      "       rics to replay, If any of the requested metric  is  not  maintained  or\n",
      "       logged  as not-supported then it's shown as \"-\" in the output. The for‐\n",
      "       mat of data produced by this mode is such that the user is running  the\n",
      "       device  monitoring  utility  interactively.  The  command line requires\n",
      "       mandatory option \"-f\" to specify complete path of the log filename, all\n",
      "       the  other  supported options are exclusive and can be used together in\n",
      "       any order.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Specify log file to be replayed\n",
      "\n",
      "       nvidia-smi replay -f <log file name>\n",
      "\n",
      "       Fetches monitoring data from the compressed log  file  and  allows  the\n",
      "       user  to  see  one  line of monitoring data (default metrics with time-\n",
      "       stamp) for each monitoring iteration stored in the log file. A new line\n",
      "       of  monitoring  data is replayed every other second irrespective of the\n",
      "       actual monitoring frequency maintained at the time of collection. It is\n",
      "       displayed till the end of file or until terminated by ^C.\n",
      "\n",
      "       2) Filter metrics to be replayed\n",
      "\n",
      "       nvidia-smi replay -f <path to log file> -s <metric_group>\n",
      "\n",
      "       <metric_group> can be one or more from the following:\n",
      "\n",
      "           p  -  Power  Usage  (in Watts) and Gpu/Memory Temperature (in C) if\n",
      "       supported\n",
      "\n",
      "           u - Utilization (SM, Memory, Encoder and Decoder Utilization in %)\n",
      "\n",
      "           c - Proc and Mem Clocks (in MHz)\n",
      "\n",
      "           v - Power Violations (in %) and Thermal Violations  (as  a  boolean\n",
      "       flag)\n",
      "\n",
      "           m - Frame Buffer and Bar1 memory usage (in MB)\n",
      "\n",
      "            e  -  ECC (Number of aggregated single bit, double bit ecc errors)\n",
      "       and PCIe Replay errors\n",
      "\n",
      "           t - PCIe Rx and Tx Throughput in MB/s (Maxwell and above)\n",
      "\n",
      "       3) Limit replay to one or more devices\n",
      "\n",
      "       nvidia-smi replay -f <log file> -i <device1,device2, .. , deviceN>\n",
      "\n",
      "       Limits reporting of the metrics to the set of devices selected by comma\n",
      "       separated device list. The tool skips any of the devices not maintained\n",
      "       in the log file.\n",
      "\n",
      "       4) Restrict the time frame between which data is reported\n",
      "\n",
      "       nvidia-smi replay -f <log file> -b <start time in HH:MM:SS  format>  -e\n",
      "       <end time in HH:MM:SS format>\n",
      "\n",
      "       This  option  allows  the data to be limited between the specified time\n",
      "       range. Specifying time as 0 with -b or -e option implies start  or  end\n",
      "       file respectively.\n",
      "\n",
      "       5) Redirect replay information to a log file\n",
      "\n",
      "       nvidia-smi replay -f <log file> -r <output file name>\n",
      "\n",
      "       This option takes log file as an input and extracts the information re‐\n",
      "       lated to default metrics in the specified output file.\n",
      "\n",
      "       6) Help Information\n",
      "\n",
      "       nvidia-smi replay -h\n",
      "\n",
      "       Displays help information for using the command line.\n",
      "\n",
      "   Process Monitoring\n",
      "       The \"nvidia-smi pmon\" command-line  is  used  to  monitor  compute  and\n",
      "       graphics  processes  running  on  one  or  more  GPUs (up to 4 devices)\n",
      "       plugged into the system. This tool allows the user to see  the  statis‐\n",
      "       tics  for  all the running processes on each device at every monitoring\n",
      "       cycle. The output is in concise format and easy to interpret in  inter‐\n",
      "       active  mode. The output data per line is limited by the terminal size.\n",
      "       It is supported on Tesla, GRID, Quadro and limited GeForce products for\n",
      "       Kepler  or  newer  GPUs under bare metal 64 bits Linux. By default, the\n",
      "       monitoring data for each process includes the pid, command name and av‐\n",
      "       erage  utilization values for SM, Memory, Encoder and Decoder since the\n",
      "       last monitoring cycle. It can also be configured to report frame buffer\n",
      "       memory  usage  for each process. If there is no process running for the\n",
      "       device, then all the metrics are reported as \"-\" for the device. If any\n",
      "       of  the  metric  is  not  supported on the device or any other error in\n",
      "       fetching the metric is also reported as \"-\" in  the  output  data.  The\n",
      "       user can also configure monitoring frequency and the number of monitor‐\n",
      "       ing iterations for each run. There is also an option  to  include  date\n",
      "       and  time at each line. All the supported options are exclusive and can\n",
      "       be used together in any order.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Default with no arguments\n",
      "\n",
      "       nvidia-smi pmon\n",
      "\n",
      "       Monitors all the processes running on each device for up to 4 supported\n",
      "       devices under natural enumeration (starting with GPU index 0) at a fre‐\n",
      "       quency of 1 sec. Runs until terminated with ^C.\n",
      "\n",
      "       2) Select one or more devices\n",
      "\n",
      "       nvidia-smi pmon -i <device1,device2, .. , deviceN>\n",
      "\n",
      "       Reports statistics for all the processes running  on  the  devices  se‐\n",
      "       lected by comma separated device list. The tool picks up to 4 supported\n",
      "       devices from the list under natural enumeration (starting with GPU  in‐\n",
      "       dex 0).\n",
      "\n",
      "       3) Select metrics to be displayed\n",
      "\n",
      "       nvidia-smi pmon -s <metric_group>\n",
      "\n",
      "       <metric_group> can be one or more from the following:\n",
      "\n",
      "           u  -  Utilization  (SM, Memory, Encoder and Decoder Utilization for\n",
      "       the process in %). Reports average utilization  since  last  monitoring\n",
      "       cycle.\n",
      "\n",
      "           m  -  Frame  Buffer  usage (in MB). Reports instantaneous value for\n",
      "       memory usage.\n",
      "\n",
      "       4) Configure monitoring iterations\n",
      "\n",
      "       nvidia-smi pmon -c <number of samples>\n",
      "\n",
      "       Displays data for specified number of samples and exit.\n",
      "\n",
      "       5) Configure monitoring frequency\n",
      "\n",
      "       nvidia-smi pmon -d <time in secs>\n",
      "\n",
      "       Collects and displays data at every specified monitoring interval until\n",
      "       terminated  with  ^C.  The monitoring frequency must be between 1 to 10\n",
      "       secs.\n",
      "\n",
      "       6) Display date\n",
      "\n",
      "       nvidia-smi pmon -o D\n",
      "\n",
      "       Prepends monitoring data with date in YYYYMMDD format.\n",
      "\n",
      "       7) Display time\n",
      "\n",
      "       nvidia-smi pmon -o T\n",
      "\n",
      "       Prepends monitoring data with time in HH:MM:SS format.\n",
      "\n",
      "       8) Help Information\n",
      "\n",
      "       nvidia-smi pmon -h\n",
      "\n",
      "       Displays help information for using the command line.\n",
      "\n",
      "   Topology (EXPERIMENTAL)\n",
      "       List topology information about the system's GPUs, how they connect  to\n",
      "       each other as well as qualified NICs capable of RDMA\n",
      "\n",
      "       Displays a matrix of available GPUs with the following legend:\n",
      "\n",
      "       Legend:\n",
      "\n",
      "                        X    = Self\n",
      "                        SYS   =  Connection traversing PCIe as well as the SMP\n",
      "                      interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "                        NODE = Connection traversing PCIe as well as  the  in‐\n",
      "                      terconnect between PCIe Host Bridges within a NUMA node\n",
      "                        PHB   =  Connection  traversing PCIe as well as a PCIe\n",
      "                      Host Bridge (typically the CPU)\n",
      "                        PXB  = Connection traversing  multiple  PCIe  switches\n",
      "                      (without traversing the PCIe Host Bridge)\n",
      "                        PIX  = Connection traversing a single PCIe switch\n",
      "                        NV#  = Connection traversing a bonded set of # NVLinks\n",
      "\n",
      "   vGPU Management\n",
      "       The  \"nvidia-smi  vgpu\" command reports on GRID vGPUs executing on sup‐\n",
      "       ported GPUs and hypervisors (refer to driver  release  notes  for  sup‐\n",
      "       ported  platforms).  Summary reporting provides basic information about\n",
      "       vGPUs currently executing on the system. Additional options provide de‐\n",
      "       tailed  reporting of vGPU properties, per-vGPU reporting of SM, Memory,\n",
      "       Encoder, and Decoder utilization, and per-GPU  reporting  of  supported\n",
      "       and creatable vGPUs. Periodic reports can be automatically generated by\n",
      "       specifying a configurable loop frequency to any command.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Help Information\n",
      "\n",
      "       nvidia-smi vgpu -h\n",
      "\n",
      "       Displays help information for using the command line.\n",
      "\n",
      "       2) Default with no arguments\n",
      "\n",
      "       nvidia-smi vgpu\n",
      "\n",
      "       Reports summary of all the vGPUs currently active on each device.\n",
      "\n",
      "       3) Display detailed info on currently active vGPUs\n",
      "\n",
      "       nvidia-smi vgpu -q\n",
      "\n",
      "       Collects and displays information on currently active vGPUs on each de‐\n",
      "       vice, including driver version, utilization, and other information.\n",
      "\n",
      "       4) Select one or more devices\n",
      "\n",
      "       nvidia-smi vgpu -i <device1,device2, .. , deviceN>\n",
      "\n",
      "       Reports  summary  for all the vGPUs currently active on the devices se‐\n",
      "       lected by comma-separated device list.\n",
      "\n",
      "       5) Display supported vGPUs\n",
      "\n",
      "       nvidia-smi vgpu -s\n",
      "\n",
      "       Displays vGPU types supported on each device. Use the  -v  /  --verbose\n",
      "       option to show detailed info on each vGPU type.\n",
      "\n",
      "       6) Display creatable vGPUs\n",
      "\n",
      "       nvidia-smi vgpu -c\n",
      "\n",
      "       Displays  vGPU types creatable on each device. This varies dynamically,\n",
      "       depending on the vGPUs already active on  the  device.  Use  the  -v  /\n",
      "       --verbose option to show detailed info on each vGPU type.\n",
      "\n",
      "       7) Report utilization for currently active vGPUs.\n",
      "\n",
      "       nvidia-smi vgpu -u\n",
      "\n",
      "       Reports  average utilization (SM, Memory, Encoder and Decoder) for each\n",
      "       active vGPU since last monitoring cycle. The default cycle  time  is  1\n",
      "       second,  and the command runs until terminated with ^C. If a device has\n",
      "       no active vGPUs, its metrics are reported as \"-\".\n",
      "\n",
      "       8) Configure loop frequency\n",
      "\n",
      "       nvidia-smi vgpu [-s -c -q -u] -l <time in secs>\n",
      "\n",
      "       Collects and displays data at a specified loop  interval  until  termi‐\n",
      "       nated  with  ^C. The loop frequency must be between 1 and 10 secs. When\n",
      "       no time is specified, the loop frequency defaults to 5 secs.\n",
      "\n",
      "       9) Display GPU engine usage\n",
      "\n",
      "       nvidia-smi vgpu -p\n",
      "\n",
      "       Display GPU engine usage of currently active processes running  in  the\n",
      "       vGPU VMs.\n",
      "\n",
      "       10) Display migration capabitlities.\n",
      "\n",
      "       nvidia-smi vgpu -m\n",
      "\n",
      "       Display pGPU's migration/suspend/resume capability.\n",
      "\n",
      "       11) Display Nvidia Encoder session info.\n",
      "\n",
      "       nvidia-smi vgpu -es\n",
      "\n",
      "       Display  the  information  about encoder sessions for currently running\n",
      "       vGPUs.\n",
      "\n",
      "       12) Display accounting statistics.\n",
      "\n",
      "       nvidia-smi vgpu --query-accounted-apps=[input parameters]\n",
      "\n",
      "       Display accounting stats for compute/graphics processes.\n",
      "\n",
      "       To find list of properties which can  be  queried,  run  -  'nvidia-smi\n",
      "       --help-query-accounted-apps'.\n",
      "\n",
      "       13) Display Nvidia Frame Buffer Capture session info.\n",
      "\n",
      "       nvidia-smi vgpu -fs\n",
      "\n",
      "       Display the information about FBC sessions for currently running vGPUs.\n",
      "\n",
      "       Note  : Horizontal resolution, vertical resolution, average FPS and av‐\n",
      "       erage latency data for a FBC session may be zero if there  are  no  new\n",
      "       frames captured since the session started.\n",
      "\n",
      "   MIG Management\n",
      "       The  privileged \"nvidia-smi mig\" command-line is used to manage MIG-en‐\n",
      "       abled GPUs. It provides options to create, list  and  destroy  GPU  in‐\n",
      "       stances and compute instances.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Display help menu\n",
      "\n",
      "       nvidia-smi mig -h\n",
      "\n",
      "       Displays help menu for using the command-line.\n",
      "\n",
      "       2) Select one or more GPUs\n",
      "\n",
      "       nvidia-smi mig -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --id <GPU IDs>\n",
      "\n",
      "       Selects  one  or more GPUs using the given comma-separated GPU indexes,\n",
      "       PCI bus IDs or UUIDs. If not used, the given  command-line  option  ap‐\n",
      "       plies to all of the supported GPUs.\n",
      "\n",
      "       3) Select one or more GPU instances\n",
      "\n",
      "       nvidia-smi mig -gi <GPU instance IDs>\n",
      "\n",
      "       nvidia-smi mig --gpu-instance-id <GPU instance IDs>\n",
      "\n",
      "       Selects  one  or more GPU instances using the given comma-separated GPU\n",
      "       instance IDs. If not used, the given command-line option applies to all\n",
      "       of the GPU instances.\n",
      "\n",
      "       4) Select one or more compute instances\n",
      "\n",
      "       nvidia-smi mig -ci <compute instance IDs>\n",
      "\n",
      "       nvidia-smi mig --compute-instance-id <compute instance IDs>\n",
      "\n",
      "       Selects  one  or more compute instances using the given comma-separated\n",
      "       compute instance IDs. If not used, the given  command-line  option  ap‐\n",
      "       plies to all of the compute instances.\n",
      "\n",
      "       5) List GPU instance profiles\n",
      "\n",
      "       nvidia-smi mig -lgip -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --list-gpu-instance-profiles --id <GPU IDs>\n",
      "\n",
      "       Lists  GPU  instance profiles, their availability and IDs. Profiles de‐\n",
      "       scribe the supported types of GPU instances, including all of  the  GPU\n",
      "       resources they exclusively control.\n",
      "\n",
      "       6) List GPU instance possible placements\n",
      "\n",
      "       nvidia-smi mig -lgipp -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --list-gpu-instance-possible-placements --id <GPU IDs>\n",
      "\n",
      "       Lists  GPU  instance  possible placements. Possible placements describe\n",
      "       the locations of the supported types of GPU instances within the GPU.\n",
      "\n",
      "       7) Create GPU instance\n",
      "\n",
      "       nvidia-smi mig -cgi <GPU instance specifiers> -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --create-gpu-instance  <GPU  instance  specifiers>  --id\n",
      "       <GPU IDs>\n",
      "\n",
      "       Creates  GPU instances for the given GPU instance specifiers. A GPU in‐\n",
      "       stance specifier comprises a GPU instance profile name or ID and an op‐\n",
      "       tional  placement specifier consisting of a colon and a placement start\n",
      "       index. The command fails if the GPU resources required to allocate  the\n",
      "       requested GPU instances are not available, or if the placement index is\n",
      "       not valid for the given profile.\n",
      "\n",
      "       8) Create a GPU instance along with the default compute instance\n",
      "\n",
      "       nvidia-smi mig -cgi <GPU instance profile IDs or names> -i <GPU IDs> -C\n",
      "\n",
      "       nvidia-smi mig  --create-gpu-instance  <GPU  instance  profile  IDs  or\n",
      "       names> --id <GPU IDs> --default-compute-instance\n",
      "\n",
      "       9) List GPU instances\n",
      "\n",
      "       nvidia-smi mig -lgi -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --list-gpu-instances --id <GPU IDs>\n",
      "\n",
      "       Lists GPU instances and their IDs.\n",
      "\n",
      "       10) Destroy GPU instance\n",
      "\n",
      "       nvidia-smi mig -dgi -gi <GPU instance IDs> -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi  mig --destroy-gpu-instances --gpu-instance-id <GPU instance\n",
      "       IDs> --id <GPU IDs>\n",
      "\n",
      "       Destroys GPU instances. The command fails if the requested GPU instance\n",
      "       is in use by an application.\n",
      "\n",
      "       11) List compute instance profiles\n",
      "\n",
      "       nvidia-smi mig -lcip -gi <GPU instance IDs> -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi  mig --list-compute-instance-profiles --gpu-instance-id <GPU\n",
      "       instance IDs> --id <GPU IDs>\n",
      "\n",
      "       Lists compute instance profiles, their availability and  IDs.  Profiles\n",
      "       describe the supported types of compute instances, including all of the\n",
      "       GPU resources they share or exclusively control.\n",
      "\n",
      "       12) Create compute instance\n",
      "\n",
      "       nvidia-smi mig -cci <compute instance profile IDs or  names>  -gi  <GPU\n",
      "       instance IDs> -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi  mig --create-compute-instance <compute instance profile IDs\n",
      "       or names> --gpu-instance-id <GPU instance IDs> --id <GPU IDs>\n",
      "\n",
      "       Creates compute instances for the given compute instance profile IDs or\n",
      "       names.  The command fails if the GPU resources required to allocate the\n",
      "       requested compute instances are not available.\n",
      "\n",
      "       13) List compute instances\n",
      "\n",
      "       nvidia-smi mig -lci -gi <GPU instance IDs> -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --list-compute-instances --gpu-instance-id <GPU instance\n",
      "       IDs> --id <GPU IDs>\n",
      "\n",
      "       Lists compute instances and their IDs.\n",
      "\n",
      "       14) Destroy compute instance\n",
      "\n",
      "       nvidia-smi  mig  -dci -ci <compute instance IDs> -gi <GPU instance IDs>\n",
      "       -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi mig --destroy-compute-instance  --compute-instance-id  <com‐\n",
      "       pute instance IDs> --gpu-instance-id <GPU instance IDs> --id <GPU IDs>\n",
      "\n",
      "       Destroys  compute instances. The command fails if the requested compute\n",
      "       instance is in use by an application.\n",
      "\n",
      "   Boost Slider\n",
      "       The privileged \"nvidia-smi boost-slider\" command-line is used to manage\n",
      "       boost  slider  on  GPUs.  It provides options to list and control boost\n",
      "       sliders.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Display help menu\n",
      "\n",
      "       nvidia-smi boost-slider -h\n",
      "\n",
      "       Displays help menu for using the command-line.\n",
      "\n",
      "       2) List one or more GPUs\n",
      "\n",
      "       nvidia-smi boost-slider -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi boost-slider --id <GPU IDs>\n",
      "\n",
      "       Selects one or more GPUs using the given comma-separated  GPU  indexes,\n",
      "       PCI  bus  IDs  or UUIDs. If not used, the given command-line option ap‐\n",
      "       plies to all of the supported GPUs.\n",
      "\n",
      "       3) List boost sliders\n",
      "\n",
      "       nvidia-smi boost-slider -l\n",
      "\n",
      "       nvidia-smi boost-slider --list\n",
      "\n",
      "       List all boost sliders for the selected devices.\n",
      "\n",
      "       4) Set video boost slider\n",
      "\n",
      "       nvidia-smi boost-slider --vboost <value>\n",
      "\n",
      "       Set the video boost slider for the selected devices.\n",
      "\n",
      "   Power Hint\n",
      "       The privileged \"nvidia-smi power-hint\" command-line is  used  to  query\n",
      "       power hint on GPUs.\n",
      "\n",
      "       Usage:\n",
      "\n",
      "       1) Display help menu\n",
      "\n",
      "       nvidia-smi boost-slider -h\n",
      "\n",
      "       Displays help menu for using the command-line.\n",
      "\n",
      "       2) List one or more GPUs\n",
      "\n",
      "       nvidia-smi boost-slider -i <GPU IDs>\n",
      "\n",
      "       nvidia-smi boost-slider --id <GPU IDs>\n",
      "\n",
      "       Selects  one  or more GPUs using the given comma-separated GPU indexes,\n",
      "       PCI bus IDs or UUIDs. If not used, the given  command-line  option  ap‐\n",
      "       plies to all of the supported GPUs.\n",
      "\n",
      "       3) List power hint info\n",
      "\n",
      "       nvidia-smi boost-slider -l\n",
      "\n",
      "       nvidia-smi boost-slider --list-info\n",
      "\n",
      "       List all boost sliders for the selected devices.\n",
      "\n",
      "       4) Query power hint\n",
      "\n",
      "       nvidia-smi boost-slider -gc <value> -t <value> -p <profile ID>\n",
      "\n",
      "       nvidia-smi  boost-slider --graphics-clock <value> --temperature <value>\n",
      "       --profile <profile ID>\n",
      "\n",
      "       Query power hint with graphics clock, temperature and profile id.\n",
      "\n",
      "       5) Query power hint\n",
      "\n",
      "       nvidia-smi boost-slider -gc <value> -mc <value> -t <value> -p  <profile\n",
      "       ID>\n",
      "\n",
      "       nvidia-smi boost-slider --graphics-clock <value> --memory-clock <value>\n",
      "       --temperature <value> --profile <profile ID>\n",
      "\n",
      "       Query power hint with graphics clock,  memory  clock,  temperature  and\n",
      "       profile id.\n",
      "\n",
      "UNIT ATTRIBUTES\n",
      "       The  following  list  describes all possible data returned by the -q -u\n",
      "       unit query option.  Unless otherwise noted all  numerical  results  are\n",
      "       base 10 and unitless.\n",
      "\n",
      "   Timestamp\n",
      "       The  current system timestamp at the time nvidia-smi was invoked.  For‐\n",
      "       mat is \"Day-of-week Month Day HH:MM:SS Year\".\n",
      "\n",
      "   Driver Version\n",
      "       The version of the installed NVIDIA display  driver.   Format  is  \"Ma‐\n",
      "       jor-Number.Minor-Number\".\n",
      "\n",
      "   HIC Info\n",
      "       Information  about any Host Interface Cards (HIC) that are installed in\n",
      "       the system.\n",
      "\n",
      "       Firmware Version\n",
      "                      The version of the firmware running on the HIC.\n",
      "\n",
      "   Attached Units\n",
      "       The number of attached Units in the system.\n",
      "\n",
      "   Product Name\n",
      "       The official product name of the unit.  This is an alphanumeric  value.\n",
      "       For all S-class products.\n",
      "\n",
      "   Product Id\n",
      "       The  product identifier for the unit.  This is an alphanumeric value of\n",
      "       the form \"part1-part2-part3\".  For all S-class products.\n",
      "\n",
      "   Product Serial\n",
      "       The immutable globally unique identifier for the unit.  This is an  al‐\n",
      "       phanumeric value.  For all S-class products.\n",
      "\n",
      "   Firmware Version\n",
      "       The version of the firmware running on the unit.  Format is \"Major-Num‐\n",
      "       ber.Minor-Number\".  For all S-class products.\n",
      "\n",
      "   LED State\n",
      "       The LED indicator is used to flag systems with potential problems.   An\n",
      "       LED color of AMBER indicates an issue.  For all S-class products.\n",
      "\n",
      "       Color          The  color of the LED indicator.  Either \"GREEN\" or \"AM‐\n",
      "                      BER\".\n",
      "\n",
      "       Cause          The reason for the current LED color.  The cause may  be\n",
      "                      listed as any combination of \"Unknown\", \"Set to AMBER by\n",
      "                      host system\", \"Thermal sensor  failure\",  \"Fan  failure\"\n",
      "                      and \"Temperature exceeds critical limit\".\n",
      "\n",
      "   Temperature\n",
      "       Temperature  readings  for important components of the Unit.  All read‐\n",
      "       ings are in degrees C.  Not all readings may be available.  For all  S-\n",
      "       class products.\n",
      "\n",
      "       Intake         Air temperature at the unit intake.\n",
      "\n",
      "       Exhaust        Air temperature at the unit exhaust point.\n",
      "\n",
      "       Board          Air temperature across the unit board.\n",
      "\n",
      "   PSU\n",
      "       Readings for the unit power supply.  For all S-class products.\n",
      "\n",
      "       State          Operating  state of the PSU.  The power supply state can\n",
      "                      be any of the  following:  \"Normal\",  \"Abnormal\",  \"High\n",
      "                      voltage\",  \"Fan  failure\", \"Heatsink temperature\", \"Cur‐\n",
      "                      rent  limit\",  \"Voltage  below  UV   alarm   threshold\",\n",
      "                      \"Low-voltage\",  \"I2C  remote  off command\", \"MOD_DISABLE\n",
      "                      input\" or \"Short pin transition\".\n",
      "\n",
      "       Voltage        PSU voltage setting, in volts.\n",
      "\n",
      "       Current        PSU current draw, in amps.\n",
      "\n",
      "   Fan Info\n",
      "       Fan readings for the unit.  A reading is  provided  for  each  fan,  of\n",
      "       which there can be many.  For all S-class products.\n",
      "\n",
      "       State          The state of the fan, either \"NORMAL\" or \"FAILED\".\n",
      "\n",
      "       Speed          For a healthy fan, the fan's speed in RPM.\n",
      "\n",
      "   Attached GPUs\n",
      "       A  list  of PCI bus ids that correspond to each of the GPUs attached to\n",
      "       the unit.  The bus ids have the form  \"domain:bus:device.function\",  in\n",
      "       hex.  For all S-class products.\n",
      "\n",
      "NOTES\n",
      "       On  Linux,  NVIDIA device files may be modified by nvidia-smi if run as\n",
      "       root.  Please see the relevant section of the driver README file.\n",
      "\n",
      "       The -a and -g arguments are now deprecated in favor of -q and  -i,  re‐\n",
      "       spectively.  However, the old arguments still work for this release.\n",
      "\n",
      "EXAMPLES\n",
      "   nvidia-smi -q\n",
      "       Query  attributes  for all GPUs once, and display in plain text to std‐\n",
      "       out.\n",
      "\n",
      "   nvidia-smi --format=csv,noheader --query-gpu=uuid,persistence_mode\n",
      "       Query UUID and persistence mode of all GPUs in the system.\n",
      "\n",
      "   nvidia-smi -q -d ECC,POWER -i 0 -l 10 -f out.log\n",
      "       Query ECC errors and power consumption for GPU 0 at a frequency  of  10\n",
      "       seconds, indefinitely, and record to the file out.log.\n",
      "\n",
      "   \"nvidia-smi                    -c                    1                   -i\n",
      "       GPU-b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8\"\n",
      "       Set   the   compute   mode   to   \"PROHIBITED\"   for   GPU   with  UUID\n",
      "       \"GPU-b2f5f1b745e3d23d-65a3a26d-097db358-7303e0b6-149642ff3d219f8587cde3a8\".\n",
      "\n",
      "   nvidia-smi -q -u -x --dtd\n",
      "       Query attributes for all Units once, and display in XML format with em‐\n",
      "       bedded DTD to stdout.\n",
      "\n",
      "   nvidia-smi --dtd -u -f nvsmi_unit.dtd\n",
      "       Write the Unit DTD to nvsmi_unit.dtd.\n",
      "\n",
      "   nvidia-smi -q -d SUPPORTED_CLOCKS\n",
      "       Display supported clocks of all GPUs.\n",
      "\n",
      "   nvidia-smi -i 0 --applications-clocks 2500,745\n",
      "       Set applications clocks to 2500 MHz memory, and 745 MHz graphics.\n",
      "\n",
      "   nvidia-smi mig -cgi 19\n",
      "       Create a MIG GPU instance on profile ID 19.\n",
      "\n",
      "   nvidia-smi mig -cgi 19:2\n",
      "       Create a MIG GPU instance on profile ID 19 at placement start index 2.\n",
      "\n",
      "   nvidia-smi boost-slider -l\n",
      "       List all boost sliders for all GPUs.\n",
      "\n",
      "   nvidia-smi boost-slider --vboost 1\n",
      "       Set vboost to value 1 for all GPUs.\n",
      "\n",
      "   nvidia-smi power-hint -l\n",
      "       List clock range, temperature range and  supported  profiles  of  power\n",
      "       hint.\n",
      "\n",
      "   nvidia-smi boost-slider -gc 1350 -t 60 -p 0\n",
      "       Query power hint with graphics clock at 1350MHz, temperature at 60C and\n",
      "       profile ID at 0.\n",
      "\n",
      "   nvidia-smi boost-slider -gc 1350 -mc 1215 -t n5 -p 1\n",
      "       Query power hint with  graphics  clock  at  1350MHz,  memory  clock  at\n",
      "       1215MHz, temperature at -5C and profile ID at 1.\n",
      "\n",
      "CHANGE LOG\n",
      "         === Known Issues ===\n",
      "\n",
      "         *  On  Linux  GPU  Reset can't be triggered when there is pending GOM\n",
      "       change.\n",
      "\n",
      "         * On Linux GPU Reset may not successfully change pending ECC mode.  A\n",
      "       full reboot may be required to enable the mode change.\n",
      "\n",
      "         *  On  Linux  platforms that configure NVIDIA GPUs as NUMA nodes, en‐\n",
      "       abling persistence mode or resetting GPUs may print  \"Warning:  persis‐\n",
      "       tence  mode  is  disabled on device\" if nvidia-persistenced is not run‐\n",
      "       ning, or if nvidia-persistenced  cannot  access  files  in  the  NVIDIA\n",
      "       driver's procfs directory for the device (/proc/driver/nvidia/gpus/<PCI\n",
      "       Config Address>/). During GPU reset and driver reload,  this  directory\n",
      "       will  be  deleted  and  recreated,  and  outstanding  references to the\n",
      "       deleted directory, such as mounts or shells, can prevent processes from\n",
      "       accessing files in the new directory.\n",
      "\n",
      "         *  === Changes between nvidia-smi v465 Update and v470 ===\n",
      "\n",
      "         * Added support to query power hint\n",
      "\n",
      "         *  === Changes between nvidia-smi v460 Update and v465 ===\n",
      "\n",
      "         * Removed support for -acp,--application-clock-permissions option\n",
      "\n",
      "         *  === Changes between nvidia-smi v450 Update and v460 ===\n",
      "\n",
      "         * Add option to specify placement when creating a MIG GPU instance.\n",
      "\n",
      "         * Added support to query and control boost slider\n",
      "\n",
      "         *  === Changes between nvidia-smi v445 Update and v450 ===\n",
      "\n",
      "         *  Added --lock-memory-clock and --reset-memory-clock command to lock\n",
      "       to closest min/max Memory clock provided and ability  to  reset  Memory\n",
      "       clock\n",
      "\n",
      "         * Allow fan speeds greater than 100% to be reported\n",
      "\n",
      "         * Added topo support to display NUMA node affinity for GPU devices\n",
      "\n",
      "         * Added support to create MIG instances using profile names\n",
      "\n",
      "         * Added support to create the default compute instance while creating\n",
      "       a GPU instance\n",
      "\n",
      "         * Added support to query and disable MIG mode on Windows\n",
      "\n",
      "         * Removed support of GPU reset(-r) command on MIG enabled vGPU guests\n",
      "\n",
      "         *  === Changes between nvidia-smi v418 Update and v445 ===\n",
      "\n",
      "         * Added support for Multi Instance GPU (MIG)\n",
      "\n",
      "         * Added support to individually reset NVLink-capable  GPUs  based  on\n",
      "       the NVIDIA Ampere architecture\n",
      "\n",
      "         *  === Changes between nvidia-smi v361 Update and v418 ===\n",
      "\n",
      "         *  Support for Volta and Turing architectures, bug fixes, performance\n",
      "       improvements, and new features\n",
      "\n",
      "         *  === Changes between nvidia-smi v352 Update and v361 ===\n",
      "\n",
      "         * Added nvlink support to expose the publicly available  NVLINK  NVML\n",
      "       APIs\n",
      "\n",
      "         * Added clocks sub-command with synchronized boost support\n",
      "\n",
      "         * Updated nvidia-smi stats to report GPU temperature metric\n",
      "\n",
      "         * Updated nvidia-smi dmon to support PCIe throughput\n",
      "\n",
      "         * Updated nvidia-smi daemon/replay to support PCIe throughput\n",
      "\n",
      "         *  Updated  nvidia-smi dmon, daemon and replay to support PCIe Replay\n",
      "       Errors\n",
      "\n",
      "         * Added GPU part numbers in nvidia-smi -q\n",
      "\n",
      "         * Removed support for exclusive thread compute mode\n",
      "\n",
      "         * Added Video (encoder/decode) clocks to the Clocks  and  Max  Clocks\n",
      "       display of nvidia-smi -q\n",
      "\n",
      "         * Added memory temperature output to nvidia-smi dmon\n",
      "\n",
      "         *  Added  --lock-gpu-clock  and  --reset-gpu-clock command to lock to\n",
      "       closest min/max GPU clock provided and reset clock\n",
      "\n",
      "         * Added --cuda-clocks to override or restore default CUDA clocks\n",
      "\n",
      "         === Changes between nvidia-smi v346 Update and v352 ===\n",
      "\n",
      "         * Added topo support to display affinities per GPU\n",
      "\n",
      "         * Added topo support to display neighboring GPUs for a given level\n",
      "\n",
      "         * Added topo support to show pathway between two given GPUs\n",
      "\n",
      "         * Added \"nvidia-smi pmon\"  command-line  for  process  monitoring  in\n",
      "       scrolling format\n",
      "\n",
      "         * Added \"--debug\" option to produce an encrypted debug log for use in\n",
      "       submission of bugs back to NVIDIA\n",
      "\n",
      "         * Fixed reporting of Used/Free memory under Windows WDDM mode\n",
      "\n",
      "         * The accounting stats is updated to include both running and  termi‐\n",
      "       nated processes. The execution time of running process is reported as 0\n",
      "       and updated to actual value when the process is terminated.\n",
      "\n",
      "         === Changes between nvidia-smi v340 Update and v346 ===\n",
      "\n",
      "         * Added reporting of PCIe replay counters\n",
      "\n",
      "         * Added support for reporting Graphics processes via nvidia-smi\n",
      "\n",
      "         * Added reporting of PCIe utilization\n",
      "\n",
      "         * Added dmon command-line for device monitoring in scrolling format\n",
      "\n",
      "         * Added daemon command-line to run in background and monitor  devices\n",
      "       as a daemon process. Generates dated log files at /var/log/nvstats/\n",
      "\n",
      "         *  Added  replay command-line to replay/extract the stat files gener‐\n",
      "       ated by the daemon tool\n",
      "\n",
      "         === Changes between nvidia-smi v331 Update and v340 ===\n",
      "\n",
      "         * Added reporting of temperature threshold information.\n",
      "\n",
      "         * Added reporting of brand information (e.g. Tesla, Quadro, etc.)\n",
      "\n",
      "         * Added support for K40d and K80.\n",
      "\n",
      "         * Added reporting of max, min and avg for  samples  (power,  utiliza‐\n",
      "       tion,  clock changes). Example commandline: nvidia-smi -q -d power,uti‐\n",
      "       lization, clock\n",
      "\n",
      "         * Added nvidia-smi stats interface  to  collect  statistics  such  as\n",
      "       power, utilization, clock changes, xid events and perf capping counters\n",
      "       with a notion of time attached to  each  sample.  Example  commandline:\n",
      "       nvidia-smi stats\n",
      "\n",
      "         *  Added  support for collectively reporting metrics on more than one\n",
      "       GPU. Used with comma separated with \"-i\" option. Example: nvidia-smi -i\n",
      "       0,1,2\n",
      "\n",
      "         *  Added  support for displaying the GPU encoder and decoder utiliza‐\n",
      "       tions\n",
      "\n",
      "         * Added nvidia-smi topo interface to display the GPUDirect communica‐\n",
      "       tion matrix (EXPERIMENTAL)\n",
      "\n",
      "         *  Added support for displayed the GPU board ID and whether or not it\n",
      "       is a multiGPU board\n",
      "\n",
      "         * Removed user-defined throttle reason from XML output\n",
      "\n",
      "         === Changes between nvidia-smi v5.319 Update and v331 ===\n",
      "\n",
      "         * Added reporting of minor number.\n",
      "\n",
      "         * Added reporting BAR1 memory size.\n",
      "\n",
      "         * Added reporting of bridge chip firmware.\n",
      "\n",
      "         === Changes between nvidia-smi v4.319 Production  and  v4.319  Update\n",
      "       ===\n",
      "\n",
      "         * Added new --applications-clocks-permission switch to change permis‐\n",
      "       sion requirements for setting and resetting applications clocks.\n",
      "\n",
      "         === Changes between nvidia-smi v4.304 and v4.319 Production ===\n",
      "\n",
      "         * Added reporting of Display Active state and  updated  documentation\n",
      "       to clarify how it differs from Display Mode and Display Active state\n",
      "\n",
      "         *  For  consistency on multi-GPU boards nvidia-smi -L always displays\n",
      "       UUID instead of serial number\n",
      "\n",
      "         * Added machine readable selective reporting. See SELECTIVE QUERY OP‐\n",
      "       TIONS section of nvidia-smi -h\n",
      "\n",
      "         *  Added  queries for page retirement information.  See --help-query-\n",
      "       retired-pages and -d PAGE_RETIREMENT\n",
      "\n",
      "         * Renamed Clock Throttle Reason User Defined Clocks  to  Applications\n",
      "       Clocks Setting\n",
      "\n",
      "         * On error, return codes have distinct non zero values for each error\n",
      "       class. See RETURN VALUE section\n",
      "\n",
      "         * nvidia-smi -i can now query information from healthy GPU when there\n",
      "       is a problem with other GPU in the system\n",
      "\n",
      "         * All messages that point to a problem with a GPU print pci bus id of\n",
      "       a GPU at fault\n",
      "\n",
      "         * New flag --loop-ms for querying information at  higher  rates  than\n",
      "       once a second (can have negative impact on system performance)\n",
      "\n",
      "         * Added queries for accounting procsses.  See --help-query-accounted-\n",
      "       apps and -d ACCOUNTING\n",
      "\n",
      "         * Added the enforced power limit to the query output\n",
      "\n",
      "         === Changes between nvidia-smi v4.304 RC and v4.304 Production ===\n",
      "\n",
      "         * Added reporting of GPU Operation Mode (GOM)\n",
      "\n",
      "         * Added new --gom switch to set GPU Operation Mode\n",
      "\n",
      "         === Changes between nvidia-smi v3.295 and v4.304 RC ===\n",
      "\n",
      "         * Reformatted non-verbose output due to user feedback.  Removed pend‐\n",
      "       ing information from table.\n",
      "\n",
      "         *  Print  out  helpful  message if initialization fails due to kernel\n",
      "       module not receiving interrupts\n",
      "\n",
      "         * Better error handling when NVML shared library is  not  present  in\n",
      "       the system\n",
      "\n",
      "         * Added new --applications-clocks switch\n",
      "\n",
      "         *  Added new filter to --display switch. Run with -d SUPPORTED_CLOCKS\n",
      "       to list possible clocks on a GPU\n",
      "\n",
      "         * When reporting free memory, calculate it from the rounded total and\n",
      "       used memory so that values add up\n",
      "\n",
      "         *  Added  reporting of power management limit constraints and default\n",
      "       limit\n",
      "\n",
      "         * Added new --power-limit switch\n",
      "\n",
      "         * Added reporting of texture memory ECC errors\n",
      "\n",
      "         * Added reporting of Clock Throttle Reasons\n",
      "\n",
      "         === Changes between nvidia-smi v2.285 and v3.295 ===\n",
      "\n",
      "         * Clearer error reporting for running commands (like changing compute\n",
      "       mode)\n",
      "\n",
      "         *  When  running  commands  on  multiple  GPUs at once N/A errors are\n",
      "       treated as warnings.\n",
      "\n",
      "         * nvidia-smi -i now also supports UUID\n",
      "\n",
      "         * UUID format changed to match UUID standard and will report  a  dif‐\n",
      "       ferent value.\n",
      "\n",
      "         === Changes between nvidia-smi v2.0 and v2.285 ===\n",
      "\n",
      "         * Report VBIOS version.\n",
      "\n",
      "         * Added -d/--display flag to filter parts of data\n",
      "\n",
      "         * Added reporting of PCI Sub System ID\n",
      "\n",
      "         * Updated docs to indicate we support M2075 and C2075\n",
      "\n",
      "         * Report HIC HWBC firmware version with -u switch\n",
      "\n",
      "         * Report max(P0) clocks next to current clocks\n",
      "\n",
      "         * Added --dtd flag to print the device or unit DTD\n",
      "\n",
      "         * Added message when NVIDIA driver is not running\n",
      "\n",
      "         * Added reporting of PCIe link generation (max and current), and link\n",
      "       width (max and current).\n",
      "\n",
      "         * Getting pending driver model works on non-admin\n",
      "\n",
      "         * Added support for running nvidia-smi on Windows Guest accounts\n",
      "\n",
      "         * Running nvidia-smi without -q command will output non verbose  ver‐\n",
      "       sion of -q instead of help\n",
      "\n",
      "         *  Fixed  parsing  of  -l/--loop=  argument (default value, 0, to big\n",
      "       value)\n",
      "\n",
      "         * Changed format of pciBusId (to XXXX:XX:XX.X - this change was visi‐\n",
      "       ble in 280)\n",
      "\n",
      "         *  Parsing  of busId for -i command is less restrictive. You can pass\n",
      "       0:2:0.0 or 0000:02:00 and other variations\n",
      "\n",
      "         * Changed versioning scheme to also include \"driver version\"\n",
      "\n",
      "         * XML format always conforms to DTD, even when error conditions occur\n",
      "\n",
      "         * Added support for single and double bit ECC events and  XID  errors\n",
      "       (enabled by default with -l flag disabled for -x flag)\n",
      "\n",
      "         * Added device reset -r --gpu-reset flags\n",
      "\n",
      "         * Added listing of compute running processes\n",
      "\n",
      "         * Renamed power state to performance state. Deprecated support exists\n",
      "       in XML output only.\n",
      "\n",
      "         * Updated DTD version number to 2.0 to match the updated XML output\n",
      "\n",
      "SEE ALSO\n",
      "       On     Linux,     the     driver     README     is     installed     as\n",
      "       /usr/share/doc/NVIDIA_GLX-1.0/README.txt\n",
      "\n",
      "AUTHOR\n",
      "       NVIDIA Corporation\n",
      "\n",
      "COPYRIGHT\n",
      "       Copyright 2011-2023 NVIDIA Corporation.\n",
      "\n",
      "nvidia-smi 470.199                 2023/5/11                     nvidia-smi(1)\n"
     ]
    }
   ],
   "source": [
    "!man nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ac6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f0201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
